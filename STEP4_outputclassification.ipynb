{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v0oKnaC0ZOh"
      },
      "source": [
        "Classification of the results as \"true\" or \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "import logging\n",
        "from openai import OpenAI\n",
        "import signal\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcP59cYJCF0_"
      },
      "source": [
        "#Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QqeshBunBXLw"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "def check_answers_regex(answer_a, answer_b):\n",
        "    \"\"\"Checks if answer_a equals answer_be after ignoring differences in capitalization or symbols\n",
        "\n",
        "    Args:\n",
        "        answer_a (str): an answer\n",
        "        answer_b (str): another answer to be compared to answer_a\n",
        "\n",
        "    Returns:\n",
        "        bool: True if both answers are the same, False otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure answers are strings\n",
        "    answer_a = str(answer_a) if answer_a is not None else \"\"\n",
        "    answer_b = str(answer_b) if answer_b is not None else \"\"\n",
        "\n",
        "    # Remove common variations and convert to lowercase\n",
        "    normalized_a = re.sub(r'(?i)answer|the solution is|[^a-zA-Z0-9]', '', answer_a).lower()\n",
        "    normalized_b = re.sub(r'(?i)answer|the solution is|[^a-zA-Z0-9]', '', answer_b).lower()\n",
        "\n",
        "    if \"true\" in [normalized_a, normalized_b] or \"false\" in [normalized_a, normalized_b]:\n",
        "        if ((normalized_a.startswith(\"yes\") or normalized_b.startswith(\"yes\")) and \"true\" in [normalized_a, normalized_b]) or ((normalized_a.startswith(\"no\") or normalized_b.startswith(\"no\")) and \"false\" in [normalized_a, normalized_b]):\n",
        "            return True\n",
        "\n",
        "    # Compare the normalized answers\n",
        "    return normalized_a == normalized_b\n",
        "\n",
        "\n",
        "def determine_type(item):\n",
        "    if isinstance(item, str):\n",
        "        item = item.strip()  # Remove leading and trailing whitespace\n",
        "        if item.replace('.', '', 1).isdigit():\n",
        "            if item.isdigit():\n",
        "                return \"int\"\n",
        "            return \"float\"\n",
        "        return \"str\"\n",
        "    elif isinstance(item, (int, float)):\n",
        "        return \"num\"\n",
        "    elif isinstance(item, bool):\n",
        "        return \"bool\"\n",
        "    else:\n",
        "        return \"other\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check_answers_regex(\"yes, this is correct\", \"True\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def create_response_GPT_base(question_prompt, basemodel, api_key=OPENAI_API_KEY):\n",
        "    \"\"\"Base function for getting responses from GPT models.\n",
        "\n",
        "    Args:\n",
        "        question_prompt (str): The question prompt\n",
        "        basemodel (str): The model to use (e.g., \"gpt-4o\")\n",
        "        api_key (str, optional): OpenAI API key. Defaults to OPENAI_KEY.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response\n",
        "    \"\"\"\n",
        "    logger.info(f\"--- create_response base : {basemodel} --- \")\n",
        "\n",
        "    response = None\n",
        "    retries = 0\n",
        "\n",
        "    while retries < 3:\n",
        "        try:\n",
        "            client = OpenAI(api_key=api_key)\n",
        "\n",
        "            resp = client.chat.completions.create(\n",
        "                model=basemodel,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                seed=1,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": question_prompt}\n",
        "                ]\n",
        "            )\n",
        "            print(\"tokens : \", resp.usage.total_tokens)\n",
        "            print(\"model used : \", resp.model)\n",
        "            response = resp.choices[0].message.content\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "            print(question_prompt)\n",
        "            time.sleep(0.100)\n",
        "            response = f\"Error occurred: {e}\"\n",
        "            retries += 1\n",
        "\n",
        "    return response\n",
        "\n",
        "def create_response_GPT4o(question_prompt, api_key=OPENAI_API_KEY):\n",
        "    return create_response_GPT_base(question_prompt, \"gpt-4o\", api_key)\n",
        "\n",
        "def create_response_GPT3(question_prompt, api_key=OPENAI_API_KEY):\n",
        "    return create_response_GPT_base(question_prompt, \"gpt-3.5-turbo\", api_key)\n",
        "\n",
        "# create_response_GPT4o(\"what is 2+2?\")\n",
        "\n",
        "LLM_dict = {\"GPT3\" : {\"LLM_function\" :create_response_GPT3},\n",
        "            \"GPT4o\" : {\"LLM_function\" :create_response_GPT4o},\n",
        "                     }\n",
        "\n",
        "def get_llm_function(LLM_name, LLM_dict=LLM_dict):\n",
        "    \"\"\"Get the corresponding LLM function based on the LLM string.\n",
        "\n",
        "    Args:\n",
        "        llm_string (str): The LLM string.\n",
        "\n",
        "    Returns:\n",
        "        function: The corresponding LLM function.\n",
        "    \"\"\"\n",
        "    if LLM_name in LLM_dict:\n",
        "        return LLM_dict[LLM_name][\"LLM_function\"]\n",
        "    else:\n",
        "        raise ValueError(f\"No LLM function found for LLM '{LLM_name}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1J524YJCIPd"
      },
      "outputs": [],
      "source": [
        "def generate_choice_dict(choices):\n",
        "    choice_dict = {}\n",
        "    for i, choice in enumerate(choices):\n",
        "        letter = chr(ord('A') + i)\n",
        "        choice_dict[letter] = choice\n",
        "    return choice_dict\n",
        "\n",
        "#print(generate_choice_dict(['simile', 'alliteration']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rg6rcce5D4L7"
      },
      "outputs": [],
      "source": [
        "#returns the file names already in the results folder to add them to the list of skipped files (to not take them into account when running functions)\n",
        "def get_skipped_files(folder_path, list_skippedfiles=[]):\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.json'):\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                list_skippedfiles.append(file_path)\n",
        "    return list_skippedfiles\n",
        "\n",
        "#list_skippedfiles = get_skipped_files(results_folder_llms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kteNlQ8CKfI"
      },
      "source": [
        "#Verification functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "835pqi5jB4q_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_answer_LLM_prompt(prompt, question, right_answer, llm_answer, choices = None, LLM_function = create_response_GPT4o):\n",
        "    \"\"\"Gives an LLM  a question, the right answer and the LLM answer, and asks if the LLM answer is correct\n",
        "\n",
        "    Args:\n",
        "        question (str): question prompt\n",
        "        right_answer (str): the right answer extracted from the benchmark\n",
        "        llm_answer (str): the LLM's answer\n",
        "\n",
        "    Returns:\n",
        "        str: an LLM's assessment on whether llm_answer is correct\n",
        "    \"\"\"\n",
        "\n",
        "    #prompt = \"Someone was asked to answer the following question : \" + str(question) + \"\\n The person answered : \" + str(llm_answer) +\" . The right answer is :\" + str(right_answer) + \" \\n\\n Did the person answer correctly ? \\nAnswer by 'yes' or 'no' . \"\n",
        "    choices_prompt = \"\"\n",
        "    if choices != None:\n",
        "        choices_prompt = \"\\nThe choices for this question are: \" + str(generate_choice_dict(choices)) + \"\\nThe given response is one of the choices. It is either the right choice, which is the ground truth, or one of the wrong ones. Select the answer that is the closest amongst these choices. \"\n",
        "\n",
        "    prompt_addition = \"\\n\\nThis is the ground truth: \" + str(right_answer) + \"\\nThis is the response you need to classify in comparison to the ground truth: ' \" + str(llm_answer) + \"'\\n\\n\" + choices_prompt\n",
        "\n",
        "    return LLM_function(prompt + prompt_addition)\n",
        "\n",
        "\n",
        "def check_answer_LLM(question, right_answer, llm_answer, choices = None, LLM_function = create_response_GPT4o):\n",
        "    \"\"\"Gives an LLM  a question, the right answer and the LLM answer, and asks if the LLM answer is correct\n",
        "\n",
        "    Args:\n",
        "        question (str): question prompt\n",
        "        right_answer (str): the right answer extracted from the benchmark\n",
        "        llm_answer (str): the LLM's answer\n",
        "\n",
        "    Returns:\n",
        "        str: an LLM's assessment on whether llm_answer is correct\n",
        "    \"\"\"\n",
        "\n",
        "    #prompt = \"Someone was asked to answer the following question : \" + str(question) + \"\\n The person answered : \" + str(llm_answer) +\" . The right answer is :\" + str(right_answer) + \" \\n\\n Did the person answer correctly ? \\nAnswer by 'yes' or 'no' . \"\n",
        "    #for case sensitive answers:\n",
        "    choices_prompt = \"\"\n",
        "    if choices != None:\n",
        "        choices_prompt = \"\\nThe choices for this question are: \" + str(generate_choice_dict(choices)) + \"\\nThe given response is correct if the case of the given response matches the case of the correct response. \"\n",
        "\n",
        "    prompt1 = (\"You are skilled at interpreting texts. Your task is to classify text snippets, specifically assessing whether they are true or false.\\nThis is the question: \" \n",
        "        + str(question) + \n",
        "        \"\\n\\nThis is the correct answer to the question: \" + str(right_answer) + \n",
        "        \"\\n\\nThis is the response you need to classify: \" + str(llm_answer) + \n",
        "        \"\"\"\\n\\nAdhere to the following instructions when categorizing the response:\\n \n",
        "            - The classification result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "            - Avoid including comments in your answer.\\n\n",
        "            - Think step by step before providing your final answer.\n",
        "            \\n\\nOutput either TRUE or FALSE\"\"\")\n",
        "\n",
        "    # prompt without the question : \n",
        "    prompt2 = (\"You are skilled at interpreting texts. Your task is to compare two text snippets, specifically assessing whether they have the same meaning. These text snippets are responses to a question, the goal is to assess whether they give the same answer to the question and respond TRUE if they do.\\n\"\n",
        "        \"\\n\\nThis is the first answer to the question: \" + str(right_answer) + \n",
        "        \"\\nThis is the second answer to the question: \" + str(llm_answer) + \n",
        "        \"\"\"\\n\\nAdhere to the following instructions:\\n \n",
        "            - The result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "            - Avoid including comments in your answer.\\n\n",
        "            - Think step by step before providing your final answer.\n",
        "            \\n\\nOutput either TRUE or FALSE\n",
        "        \\n\\nExamples: -'The correct option is San Francisco.' compared to 'san francisco' : TRUE \\n\n",
        "        -'John would need 8 dollars' compared to '$8' : TRUE \\n\n",
        "        -'It would take 5 minutes' compared to '8 minutes' : FALSE\\n\n",
        "        -'The correct option is candle' compared to 'lightbulb' : FALSE \\n\"\"\")\n",
        "\n",
        "    # prompt without the question but with the choices: \n",
        "    prompt3 = ( \"You are skilled at classifying texts. Your task is to compare two text snippets. These text snippets are responses to a question, one is the correct answer, and one is an individual's answer. The goal is to classify whether the individual gave the correct answer to the question and respond TRUE if they did, FALSE otherwise.\\n\\n\" +\n",
        "        \"\\nA question was given to an individual.\\nThis is the right answer to the question: \" + str(right_answer) + \n",
        "        \"\\nThis is the individual's answer to the question: \" + str(llm_answer) + choices_prompt +\n",
        "        \"\\nIs this individual correct? \" +\n",
        "        \"\"\"\\n\\nAdhere to the following instructions:\\n \n",
        "            - The result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "            - Avoid including comments in your answer.\\n\n",
        "            \\n\\nOutput either TRUE or FALSE\n",
        "        \\n\\nExamples: - The individual's answer is 'The correct option is San Francisco.',the right answer is 'san francisco' : TRUE \\n\n",
        "        - The individual's answer is 'John would need 8 dollars', the right answer is '$8' : TRUE \\n\n",
        "        - The individual's answer is 'It would take 5 minutes', the right answer is '8 minutes' : FALSE\\n\n",
        "        - The individual's answer is 'The correct option is candle', the right answer is 'lightbulb' : FALSE \\n\"\"\")\n",
        "\n",
        "    prompt7 = ( \"Your task is to classify responses to questions, specifically assessing whether they are true or false in comparison to a ground truth.\" +  \n",
        "    \"\"\"\\n\\nAdhere to the following instructions when categorizing the response:\\n \n",
        "    - The classification result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "    - Only respond by saying 'TRUE' or 'FALSE'\\n\n",
        "    - Avoid including comments in your answer.\\n\n",
        "    - If the given response includes a response followed by an explanation, only take into account the response.\\n\n",
        "    \\n\\nExamples: - The given response is 'The correct option is San Francisco.', the ground truth is 'san francisco' : TRUE \\n\n",
        "        - The given response is 'John would need 8 dollars', the ground truth is '$8' : TRUE \\n\n",
        "        - The given response is 'The garden will be lit for 5 hours', the ground truth is '5' : TRUE \\n\n",
        "        - The given response is 'It would take 5 minutes', the ground truth is '8 minutes' : FALSE\\n\n",
        "        - The given response is 'The correct option is candle', the ground truth is 'lightbulb' : FALSE \\n\n",
        "        - The given response is 'True. Donna will successfully become an artist because she has all the skills for it.', the ground truth is 'True' : TRUE \\n\"\"\"\n",
        "    + \"\\n\\nThis is the ground truth: \" + str(right_answer) + \"\\nThis is the response you need to classify in comparison to the ground truth: ' \" + str(llm_answer) + \"'\\n\\n\" + choices_prompt )\n",
        "\n",
        "    \n",
        "    verification_prompt_neutral= (\"Your task is to classify a response to a question, specifically assessing whether the given response is true or false in comparison to the correct response.\"\n",
        "                                  + \"\\n\\nThis is the question:\" + str(question) + \"\\n\\nThis is the given response: \" + str(llm_answer) +\" \\n\\nThis is the correct response: \" + str(right_answer) +\n",
        "                                  \"\"\"\\n\\nAdhere to the following rules when classifying the given response:\\n\n",
        "    - The classification result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "    - Only output 'TRUE' or 'FALSE'\\n\n",
        "    - Never include any other comments or strings other than TRUE or FALSE in your output.\n",
        "    - Do not answer the question.\\n\n",
        "    - The given response can be phrased differently from the correct response. This does not mean it should be classified as FALSE.\\n\n",
        "    - Only assess if the given response contains the information that matches with the information provided in the correct response.\\n\n",
        "    - Check whether the correct response is in essence occurring somewhere in the given response. If so, classify it as TRUE.\\n\n",
        "    \"\"\")\n",
        "\n",
        "    verification_prompt_neutral_noquestion= (\"Your task is to classify a response to a question, specifically assessing whether the given response is true or false in comparison to the correct response.\"\n",
        "                                  + \"\\n\\nThis is the given response: \" + str(llm_answer) +\" \\n\\nThis is the correct response: \" + str(right_answer) +\n",
        "                                  \"\"\"\\n\\nAdhere to the following rules when classifying the given response:\\n\n",
        "    - The classification result should be a boolean value, represented as either TRUE or FALSE.\\n\n",
        "    - Only output 'TRUE' or 'FALSE'\\n\n",
        "    - Never include any other comments or strings other than TURE or FALSE in your output.\n",
        "    - Do not answer the question.\\n\n",
        "    - The given response can be phrased differently from the correct response. This does not mean it should be classified as FALSE.\\n\n",
        "    - Only assess if the given response contains the information that matches with the information provided in the correct response.\\n\n",
        "    - Check whether the correct response is in essence occurring somewhere in the given response. If so, classify it as TRUE.\\n\n",
        "    - Focus on the relevant information in the given response, which is often at the end of it. Do not consider non-essential information such as greetings, small talk, and comments about hobbies or skills when classifying the given response.\\n\n",
        "    \"\"\")\n",
        "    \n",
        "    \n",
        "    prompt = verification_prompt_neutral\n",
        "    prompt_num = 8\n",
        "    \n",
        "\n",
        "\n",
        "    return LLM_function(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Example usage\\npath = \"./simplified_folder/prefixes_experiment/prefixes-results150questions/prefixes_answers_short.xlsx\"\\n#prompt = \"Please verify the following answers:\"\\nLLM_name = \"Claude3\"\\nlist_skippedfiles = []\\n#add_new_verify_all_files_prompt(prompt, path, LLM_name, list_skippedfiles)\\nnew_verify_all_files_in_folder_prompt(path, LLM_name, list_skippedfiles)'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def classify_answers_in_file(prompt, file_path, new_file_path, LLM_name, list_skippedfiles):\n",
        "    #deception classification\n",
        "\n",
        "    print(\"entering function classify_answers_in_file\")\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(\"Classifying answers for \", filename, new_file_path.replace(\"//\", \"/\"), \" with \", LLM_name)\n",
        "\n",
        "    data = load_data(file_path)\n",
        "    iscorrect_column_name = f\"iscorrect_{LLM_name}\"\n",
        "    verif_column_name = f\"verif_{LLM_name}\"\n",
        "\n",
        "    if (new_file_path.replace(\"//\", \"/\") in list_skippedfiles) and (verif_column_name in data.get(\"1\", {})):\n",
        "        print(\"Skipped verification : \", verif_column_name , \"already filled in. \\n\")\n",
        "    else:\n",
        "        new_data = {}\n",
        "        questions_count = 1\n",
        "\n",
        "        for key, values in data.items():\n",
        "            llm_answer = values.get(\"llm_answer\")\n",
        "            right_answer = values.get(\"right_answer\")\n",
        "            choices = values.get(\"choices\")\n",
        "\n",
        "            is_correct, verification = verify_llm_answer(LLM_name, prompt, llm_answer, right_answer, choices, values)\n",
        "\n",
        "            new_data[key] = {\n",
        "                **values,\n",
        "                iscorrect_column_name: is_correct,\n",
        "                verif_column_name: verification\n",
        "            }\n",
        "\n",
        "        # Write the updated data back to a new or the same JSON file\n",
        "        with open(new_file_path, 'w') as new_json_file:\n",
        "            json.dump(new_data, new_json_file, indent=2)\n",
        "\n",
        "\n",
        "def verify_all_files(prompt, path, LLM_name=\"GPT4o\", list_skippedfiles=[]):\n",
        "    if os.path.isfile(path) and path.endswith('.json'):\n",
        "        # If it's a single JSON file, process it directly\n",
        "        print(f\"--- Verifying answers for {os.path.basename(path)} with {LLM_name} ---\")\n",
        "        verify_answers_in_file(prompt, path, path, LLM_name, list_skippedfiles)\n",
        "    elif os.path.isdir(path):\n",
        "        # If it's a directory, iterate over all JSON files in it\n",
        "        file_list = os.listdir(path)\n",
        "        num_files = len([f for f in file_list if f.endswith('.json')])\n",
        "\n",
        "        for i, file_name in enumerate(file_list, 1):\n",
        "            file_path = os.path.join(path, file_name)\n",
        "            if os.path.isfile(file_path) and file_name.endswith('.json'):\n",
        "                print(f\" --- Verification : File {i}/{num_files} ---\")\n",
        "                verify_answers_in_file(prompt, file_path, file_path, LLM_name, list_skippedfiles)\n",
        "    else:\n",
        "        print(f\"The path provided does not point to a valid JSON file or directory: {path}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"# Example usage\n",
        "path = \"./simplified_folder/prefixes_experiment/prefixes-results150questions/prefixes_answers_short.xlsx\"\n",
        "#prompt = \"Please verify the following answers:\"\n",
        "LLM_name = \"Claude3\"\n",
        "list_skippedfiles = []\n",
        "#add_new_verify_all_files_prompt(prompt, path, LLM_name, list_skippedfiles)\n",
        "new_verify_all_files_in_folder_prompt(path, LLM_name, list_skippedfiles)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "K1Y_2PjN0QBc",
        "outputId": "a1476aeb-3032-4849-bcc3-cb98ce76915d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Example usage\\npath = \"./simplified_folder/prefixes_experiment/prefixes-results150questions/prefixes_answers_short.xlsx\"\\n#prompt = \"Please verify the following answers:\"\\nLLM_name = \"Claude3\"\\nlist_skippedfiles = []\\n#add_new_verify_all_files_prompt(prompt, path, LLM_name, list_skippedfiles)\\nnew_verify_all_files_in_folder_prompt(path, LLM_name, list_skippedfiles)'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def verify_answers_in_file(prompt, file_path, new_file_path, LLM_name, list_skippedfiles):\n",
        "\n",
        "    print(\"entering function add_new_verify_answers_prompt\")\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(\"Verifying answers for \", filename, new_file_path.replace(\"//\", \"/\"), \" with \", LLM_name)\n",
        "\n",
        "    data = load_data(file_path)\n",
        "    iscorrect_column_name = f\"iscorrect_{LLM_name}\"\n",
        "    verif_column_name = f\"verif_{LLM_name}\"\n",
        "\n",
        "    if (new_file_path.replace(\"//\", \"/\") in list_skippedfiles) and (verif_column_name in data.get(\"1\", {})):\n",
        "        print(\"Skipped verification : \", verif_column_name , \"already filled in. \\n\")\n",
        "    else:\n",
        "        new_data = {}\n",
        "        questions_count = 1\n",
        "\n",
        "        for key, values in data.items():\n",
        "            llm_answer = values.get(\"llm_answer\")\n",
        "            right_answer = values.get(\"right_answer\")\n",
        "            choices = values.get(\"choices\")\n",
        "\n",
        "            is_correct, verification = verify_llm_answer(LLM_name, prompt, llm_answer, right_answer, choices, values)\n",
        "\n",
        "            new_data[key] = {\n",
        "                **values,\n",
        "                iscorrect_column_name: is_correct,\n",
        "                verif_column_name: verification\n",
        "            }\n",
        "\n",
        "        # Write the updated data back to a new or the same JSON file\n",
        "        with open(new_file_path, 'w') as new_json_file:\n",
        "            json.dump(new_data, new_json_file, indent=2)\n",
        "\n",
        "\n",
        "def verify_all_files(prompt, path, LLM_name=\"GPT4o\", list_skippedfiles=[]):\n",
        "    if os.path.isfile(path) and path.endswith('.json'):\n",
        "        # If it's a single JSON file, process it directly\n",
        "        print(f\"--- Verifying answers for {os.path.basename(path)} with {LLM_name} ---\")\n",
        "        verify_answers_in_file(prompt, path, path, LLM_name, list_skippedfiles)\n",
        "    elif os.path.isdir(path):\n",
        "        # If it's a directory, iterate over all JSON files in it\n",
        "        file_list = os.listdir(path)\n",
        "        num_files = len([f for f in file_list if f.endswith('.json')])\n",
        "\n",
        "        for i, file_name in enumerate(file_list, 1):\n",
        "            file_path = os.path.join(path, file_name)\n",
        "            if os.path.isfile(file_path) and file_name.endswith('.json'):\n",
        "                print(f\" --- Verification : File {i}/{num_files} ---\")\n",
        "                verify_answers_in_file(prompt, file_path, file_path, LLM_name, list_skippedfiles)\n",
        "    else:\n",
        "        print(f\"The path provided does not point to a valid JSON file or directory: {path}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"# Example usage\n",
        "path = \"./simplified_folder/prefixes_experiment/prefixes-results150questions/prefixes_answers_short.xlsx\"\n",
        "#prompt = \"Please verify the following answers:\"\n",
        "LLM_name = \"Claude3\"\n",
        "list_skippedfiles = []\n",
        "#add_new_verify_all_files_prompt(prompt, path, LLM_name, list_skippedfiles)\n",
        "new_verify_all_files_in_folder_prompt(path, LLM_name, list_skippedfiles)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0nQgs-gTtssz"
      },
      "outputs": [],
      "source": [
        "def verify_all_files_in_folder(base_path, benchmark_dict, LLM_name=\"GPT4\", list_skippedfiles=[]):\n",
        "    \"\"\"\n",
        "    Goes through all files in a folder and its subfolders, launching a verification function\n",
        "    based on the benchmark name present in the file name.\n",
        "\n",
        "    Args:\n",
        "    - base_path (str): The path to the base folder to start the search from.\n",
        "    - benchmark_dict (dict): A dictionary with benchmark names as keys and their associated\n",
        "                             verification prompts and other properties.\n",
        "    - LLM_name (str): The name of the language model to use for verification.\n",
        "    - list_skippedfiles (list): A list of files to skip during the verification process.\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file_name in files:\n",
        "            for benchmark, properties in benchmark_dict.items():\n",
        "                if benchmark in file_name and file_name.endswith('.json'):\n",
        "                    file_path = os.path.join(root, file_name)\n",
        "                    verification_prompt = properties['verifprompt']\n",
        "                    #print(verification_prompt, file_path)\n",
        "                    #print(\"passage verification prompt\")\n",
        "                    verify_all_files(verification_prompt, file_path, LLM_name, list_skippedfiles)\n",
        "                    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ast  \n",
        "\n",
        "def extract_last_part(input_string):\n",
        "    delimiter_exists = True\n",
        "    delimiter = \"####\"\n",
        "    if delimiter in input_string:\n",
        "        extracted_answer = input_string.split(delimiter)[-1].strip()\n",
        "        if extracted_answer.strip() == \"\":\n",
        "            return input_string, delimiter_exists\n",
        "        return input_string.split(delimiter)[-1].strip(),delimiter_exists\n",
        "    else:\n",
        "        delimiter_exists = False\n",
        "        return input_string, delimiter_exists\n",
        "\n",
        "\n",
        "def verify_llm_answers_in_excel_extracted_answer(excel_file_path, output_excel_file_path, LLM_name, benchmark_dict, randomly_selected_value=\"1\", save_interval=10, backup_interval=100, selected_benchmarks=[], selected_prompt_characteristics=[]):\n",
        "    \"\"\"\n",
        "    Verifies LLM answers in an Excel file and updates the `is_correct` column for each row. Saves the file periodically and creates backup files.\n",
        "\n",
        "    Args:\n",
        "        excel_file_path (str): The path to the input Excel file.\n",
        "        output_excel_file_path (str): The path to save the updated Excel file.\n",
        "        LLM_name (str): The name of the LLM model used for the verification.\n",
        "        benchmark_dict (dict): Dictionary containing benchmark information.\n",
        "        save_interval (int): The number of rows to process before saving the file. Default is 10.\n",
        "        backup_interval (int): The number of rows to process before saving a backup file. Default is 100.\n",
        "        benchmarks_to_ignore (list): List of benchmark names to ignore. Default is [].\n",
        "    \"\"\"\n",
        "    # Load the Excel file into a DataFrame\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "\n",
        "    # Define the column names based on the LLM name\n",
        "    answer_column = f'{LLM_name}_answer'\n",
        "    is_correct_column = f'{LLM_name}_is_correct'\n",
        "    verif_column = f'{LLM_name}_verif'\n",
        "    extracted_answer_column = f'{LLM_name}_extracted_answer'\n",
        "\n",
        "    # Ensure the is_correct_column and verif_column exist\n",
        "    if is_correct_column not in df.columns:\n",
        "        df[is_correct_column] = None\n",
        "    if verif_column not in df.columns:\n",
        "        df[verif_column] = None\n",
        "    if extracted_answer_column not in df.columns:\n",
        "        df[extracted_answer_column] = None\n",
        "\n",
        "    # If selected_benchmarks is empty, select all unique benchmarks\n",
        "    if not selected_benchmarks:\n",
        "        selected_benchmarks = df['benchmark'].unique().tolist()\n",
        "\n",
        "    # If selected_prompt_characteristics is empty, select all unique user characteristics\n",
        "    if not selected_prompt_characteristics:\n",
        "        selected_prompt_characteristics = df['prompt_characteristic'].unique().tolist()\n",
        "\n",
        "\n",
        "    # Filter rows where `randomly_selected` equals the specified value\n",
        "    #selected_rows = df[df['randomly_selected'] == float(randomly_selected_value)]\n",
        "    selected_rows = df[\n",
        "        (df['randomly_selected'] == float(randomly_selected_value)) &\n",
        "        (df['benchmark'].isin(selected_benchmarks)) &\n",
        "        (df['prompt_characteristic'].isin(selected_prompt_characteristics))\n",
        "    ]\n",
        "\n",
        "    # Process each row in the DataFrame\n",
        "    for i, (index, row) in enumerate(selected_rows.iterrows()):\n",
        "        if (pd.isna(row[is_correct_column]) or row[is_correct_column] == \"To be determined\" or row[is_correct_column] == \"undetermined\" or (isinstance(row[is_correct_column], str) and row[is_correct_column].startswith(\"Error \"))):\n",
        "            #if (pd.isna(row[is_correct_column]) or row[is_correct_column] == \"To be determined\" or row[is_correct_column].startswith(\"Error \")):\n",
        "            benchmark = row['benchmark']\n",
        "            \"\"\"if benchmark in benchmarks_to_ignore:\n",
        "                print(f\"Skipping row {index} due to benchmark '{benchmark}' in benchmarks_to_ignore.\")\n",
        "                continue\"\"\"\n",
        "            \n",
        "            print(f\"Processing row {i + 1}/{len(selected_rows)}\")\n",
        "            if str(row[is_correct_column]).startswith(\"Error \"):\n",
        "                print(\"error\")\n",
        "            question_prompt = row['question']  # Assuming there's a column 'question' with the prompt\n",
        "            llm_answer = str(row[answer_column]) if not pd.isna(row[answer_column]) else \"\"\n",
        "            right_answer = row['right_answer']  # Assuming there's a column 'right_answer' with the correct answer\n",
        "            choices = ast.literal_eval(row['choices']) if 'choices' in row and pd.notna(row['choices']) else []\n",
        "\n",
        "            # Determine the benchmark and corresponding verification prompt\n",
        "            prompt = benchmark_dict.get(benchmark, {}).get('verifprompt', \"\")\n",
        "\n",
        "            extracted_answer, delimiter_exists = extract_last_part(llm_answer)\n",
        "            if not delimiter_exists:\n",
        "                print(\"no delimiter\"), row['question_id'], row['persona']\n",
        "\n",
        "            values = {\n",
        "                \"question\": question_prompt,\n",
        "                \"llm_answer\": llm_answer,\n",
        "                \"right_answer\": right_answer,\n",
        "                \"choices\": choices,\n",
        "                \"extracted_answer\": extracted_answer \n",
        "            }\n",
        "\n",
        "            is_correct, verification = verify_llm_answer2(\n",
        "                LLM_name=\"GPT4o\",\n",
        "                prompt=prompt,\n",
        "                llm_answer=extracted_answer,\n",
        "                right_answer=right_answer,\n",
        "                choices=choices,\n",
        "                values=values\n",
        "            )\n",
        "\n",
        "            # Update the DataFrame with the verification result\n",
        "            df.at[index, is_correct_column] = is_correct\n",
        "            df.at[index, verif_column] = verification\n",
        "            df.at[index, extracted_answer_column] = extracted_answer if delimiter_exists else \"\"\n",
        "\n",
        "            # Save the DataFrame every `save_interval` rows\n",
        "            if (i + 1) % save_interval == 0:\n",
        "                df.to_excel(output_excel_file_path, index=False)\n",
        "                print(f\"Saved progress to {output_excel_file_path} after processing {i + 1} rows.\")\n",
        "\n",
        "            # Save a backup file every `backup_interval` rows\n",
        "            if (i + 1) % backup_interval == 0:\n",
        "                backup_file_path = output_excel_file_path.replace(\".xlsx\", f\"_backup_{i + 1}.xlsx\")\n",
        "                df.to_excel(backup_file_path, index=False)\n",
        "                print(f\"Saved backup to {backup_file_path} after processing {i + 1} rows.\")\n",
        "\n",
        "    # Final save to ensure all remaining rows are saved\n",
        "    df.to_excel(output_excel_file_path, index=False)\n",
        "    print(f\"Final save completed. Updated Excel file saved to {output_excel_file_path}\")\n",
        "\n",
        "\n",
        "def verify_llm_answer2(LLM_name, prompt, llm_answer = \"\", right_answer = \"\", choices = \"\", values = \"\"):\n",
        "    # rule based verification function + check with LLM if rules fail\n",
        "    right_answer = str(right_answer)\n",
        "\n",
        "    is_correct = \"undetermined\"\n",
        "    verification = \"\"\n",
        "\n",
        "    # if None: \n",
        "    if not llm_answer:\n",
        "        is_correct = \"Error - Nonetype\"\n",
        "        verification = \"Error occurred - Nonetype\"\n",
        "        return is_correct, verification\n",
        "    \n",
        "    # if None for right_answer: \n",
        "    if not right_answer:\n",
        "        is_correct = \"Error - No right answer\"\n",
        "        verification = \"Error occurred - no right answer\"\n",
        "        return is_correct, verification\n",
        "    \n",
        "    # if error: \n",
        "    if llm_answer.startswith(\"Error occurred\"):\n",
        "        is_correct = \"Error\"\n",
        "        verification = \"Error occurred\"\n",
        "        return is_correct, verification\n",
        "\n",
        "    \n",
        "    # if the llm_answer is the right_answer, with case sensitivity:\n",
        "    if choices:\n",
        "        for i in range(1,len(choices)):\n",
        "            if check_answers_regex(choices[i], choices[0]): #if there is some case sensitivity in the choices\n",
        "                print(\"--- case sensitivity ---\", choices[i], choices[0])\n",
        "\n",
        "                if right_answer == llm_answer:\n",
        "                    is_correct = \"True\"\n",
        "                    verification = \"correct answer\"\n",
        "                else:\n",
        "                    ######\n",
        "                    is_correct = str(check_answers_regex(\"True\", check_answer_LLM(values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], values[\"choices\"], LLM_function = create_response_GPT4o))) \n",
        "                    return is_correct, \"case sensitive\"\n",
        "        \n",
        "    # check if llm_answer is the right_answer, with no case sensitivity\n",
        "    if check_answers_regex(right_answer, llm_answer):\n",
        "        is_correct = \"True\"\n",
        "        verification = \"correct answer\"\n",
        "\n",
        "    # if the right_answer is a boolean, check if this boolean or the opposite appears in the first 5 characters of the llm_answer\n",
        "    # (for answers with a justification, such as \"false. John had 4 dollars left\")\n",
        "    elif determine_type(right_answer) == \"bool\":\n",
        "        #correct boolean\n",
        "        if check_answers_regex(right_answer, llm_answer[0:5]):\n",
        "            is_correct = \"True\"\n",
        "            verification = \"correct boolean\"\n",
        "        # wrong boolean\n",
        "        elif determine_type(llm_answer[0:5]) == \"bool\":\n",
        "            is_correct = \"False\"\n",
        "            verification = \"wrong boolean\"\n",
        "        # TO ADD if True in response and not False\n",
        "\n",
        "        \n",
        "    # if llm_answer is in the choices but it isn't the same answer and right_answer\n",
        "    elif choices and not check_answers_regex(llm_answer, right_answer):\n",
        "        for i in range(len(choices)):\n",
        "            if check_answers_regex(choices[i], llm_answer):\n",
        "                is_correct = \"False\"\n",
        "                verification = \"The answer was the wrong choice\"\n",
        "    \n",
        "    \"\"\"#if the right answer is a number: assessing whether this number is in the given answer\n",
        "    elif determine_type(right_answer) == \"num\":\n",
        "        # counting the number of numbers in the answer\n",
        "        numbers_in_llm_answer = re.findall(r'\\d+', llm_answer)\n",
        "        num_numbers = len(numbers_in_llm_answer)\n",
        "        if num_numbers != 1 : \n",
        "            verification = \"human check needed - several numbers\"\n",
        "\n",
        "        elif (re.findall(r'\\d+', right_answer) == numbers_in_llm_answer):\n",
        "            is_correct = \"True\"\n",
        "            verification = \"human check needed - correct number in answer\"\n",
        "        else:\n",
        "            is_correct = \"False\"\n",
        "            verification = \"human check needed - correct number not in answer\"\n",
        "\n",
        "    \"\"\"\n",
        "    # --- end general rule based verification -- adapted ---------------------------------\n",
        "\n",
        "    if is_correct == \"undetermined\":\n",
        "        \n",
        "        #if True:\n",
        "        #extracted_answer = extract_answer(values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], values[\"choices\"])\n",
        "        #is_correct, verification = rule_based_verification(values[\"right_answer\"], extracted_answer, values[\"choices\"])\n",
        "        #verification = \"extracted_answer : \" + verification \n",
        "\n",
        "        #if is_correct == \"undetermined\":\n",
        "        add_llmcomparison = True\n",
        "        #is_correct = str(check_answers_regex(\"True\", check_answer_LLM_prompt(prompt, values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], LLM_function = create_response_GPT4o)))\n",
        "        #is_correct = str(check_answers_regex(\"True\", check_answer_LLM_prompt(prompt, values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], LLM_function = get_llm_function(LLM_name))))\n",
        "        is_correct = str(check_answers_regex(\"True\", check_answer_LLM(values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], LLM_function = create_response_GPT4o))) \n",
        "        verification = \"went through check_answer_LLM with \" + LLM_name\n",
        "\n",
        "        #is_correct_llm2 = str(check_answers_regex(\"True\", check_answer_LLM(values[\"question\"], values[\"right_answer\"], values[\"llm_answer\"], LLM_function = create_response_GPT3)))\n",
        "    return is_correct, verification\n",
        "\n",
        "\n",
        "#with same verifprompt : \n",
        "benchmark_dict = {\n",
        "    \"commonsenseQA\": {\"verifprompt\": \"verification_prompt_neutral\", \"restructured_path\": \"./simplified_folder/restructured_benchmarks/restructured_commonsenseQA.json\", \"small_path\": \"./simplified_folder/benchmarks/small-commonsenseQA.json\", \"rephrased_path\": \"./simplified_folder/rephrased_benchmarks/rephrased_commonsenseQA.json\"},\n",
        "    \"numGLUE\": {\"verifprompt\": \"verification_prompt_neutral\", \"restructured_path\": \"./simplified_folder/restructured_benchmarks/restructured_numglue.json\", \"small_path\": \"./simplified_folder/benchmarks/small-numglue.json\"},\n",
        "    \"scienceQA\": {\"verifprompt\": \"verification_prompt_neutral\", \"restructured_path\": \"./simplified_folder/restructured_benchmarks/restructured_scienceQA.json\", \"small_path\": \"./simplified_folder/benchmarks/small-scienceQA.json\"},\n",
        "    \"strategyQA\": {\"verifprompt\": \"verification_prompt_neutral\", \"restructured_path\": \"./simplified_folder/restructured_benchmarks/restructured_strategyQA.json\", \"small_path\": \"./simplified_folder/benchmarks/small-strategyQA.json\"},\n",
        "    \"CRT\": {\"verifprompt\": \"verification_prompt_neutral\", \"restructured_path\": \"./simplified_folder/restructured_benchmarks/restructured_CRT.json\", \"small_path\": \"./simplified_folder/benchmarks/CRT.json\"}\n",
        "}\n"
      ]
    },
    
      ],
      "source": [
        "\n",
        "#OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "folder_path = \"YOU_FOLDER_PATH\"\n",
        "folder_modified_benchmarks_step2 = folder_path + \"step2_modified-benchmarks/\"\n",
        "folder_answered_benchmarks_step3 = folder_path + \"step3_answered-benchmarks/\"\n",
        "modified_benchmark_path = folder_modified_benchmarks_step2 + \"strategyQA_base_base.json\"\n",
        "merged_benchmarks_path = folder_modified_benchmarks_step2 + \"merged_benchmarks.xlsx\"\n",
        "verified_benchmarks_path = folder_answered_benchmarks_step3 + \"verified_benchmarks.xlsx\"\n",
        "\n",
        "LLM_dict = {\"GPT3\" : {\"LLM_function\" :create_response_GPT3},\n",
        "            \"GPT4o\" : {\"LLM_function\" :create_response_GPT4o},\n",
        "                     }\n",
        "\n",
        "LLM_to_verify = 'GPT4o'\n",
        "verify_llm_answers_in_excel_extracted_answer(merged_benchmarks_path, verified_benchmarks_path, LLM_to_verify, benchmark_dict, save_interval=50, backup_interval=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def relaunch_verification_errors(file_path, output_excel_file_path):\n",
        "    \"\"\"\n",
        "    Process the Excel file by iterating over columns ending with '_answer' and rows that\n",
        "    are empty or contain 'Error' or 'error'. Calls a placeholder function for each identified row.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the input Excel file.\n",
        "    \"\"\"\n",
        "    # Load the Excel file into a DataFrame\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Identify columns that end with '_answer' and are not 'right_answer'\n",
        "    answer_columns = [col for col in df.columns if col.endswith('_is_correct') and not col.startswith('llama')]\n",
        "    errors_list = []  # To store task_id - model combinations with errors or empty values\n",
        "\n",
        "    # Iterate over each identified column\n",
        "    for col in answer_columns:\n",
        "        model_name = col.replace('_is_correct', '')  # Extract the model name from the column name\n",
        "        #llm_function = get_llm_function(model_name)\n",
        "\n",
        "        # Check if the columns for the model exist, if not create them\n",
        "        #answer_column = f'{model_name}_answer'\n",
        "        #is_correct_column = f'{model_name}_is_correct'\n",
        "        #date_column = f'date_of_launch_{model_name}'\n",
        "        \n",
        "        \"\"\"if answer_column not in df.columns:\n",
        "            df[answer_column] = None\n",
        "        if is_correct_column not in df.columns:\n",
        "            df[is_correct_column] = None\n",
        "        if date_column not in df.columns:\n",
        "            df[date_column] = None\"\"\"\n",
        "\n",
        "        # Iterate over each row in the current column using items()\n",
        "        for index, value in df[col].items():\n",
        "            if pd.isna(value) or 'Error occured' in str(value) or str(value) == 'To be determined' or str(value) == 'undetermined':\n",
        "                # If the row is empty or contains 'Error'/'error', process it\n",
        "                question_prompt = df.at[index, 'question']  # Get the associated question\n",
        "                task_id = df.at[index, 'task_id']    # Get the associated task_id\n",
        "                print(f\"Empty or error row detected at index {index}, model: {model_name}, task_id: {task_id}\")\n",
        "                \n",
        "                benchmark = row['benchmark']\n",
        "                \"\"\"if benchmark in benchmarks_to_ignore:\n",
        "                print(f\"Skipping row {index} due to benchmark '{benchmark}' in benchmarks_to_ignore.\")\n",
        "                continue\"\"\"\n",
        "                \n",
        "                print(f\"Processing row {i + 1}/{len(selected_rows)}\")\n",
        "                question_prompt = row['question']  # Assuming there's a column 'question' with the prompt\n",
        "                llm_answer = str(row[answer_column]) if not pd.isna(row[answer_column]) else \"\"\n",
        "                right_answer = row['right_answer']  # Assuming there's a column 'right_answer' with the correct answer\n",
        "                choices = ast.literal_eval(row['choices']) if 'choices' in row and pd.notna(row['choices']) else []\n",
        "\n",
        "                # Determine the benchmark and corresponding verification prompt\n",
        "                prompt = benchmark_dict.get(benchmark, {}).get('verifprompt', \"\")\n",
        "\n",
        "                extracted_answer, delimiter_exists = extract_last_part(llm_answer)\n",
        "                if not delimiter_exists:\n",
        "                    print(\"no delimiter\"), row['task_id']\n",
        "\n",
        "                values = {\n",
        "                    \"question\": question_prompt,\n",
        "                    \"llm_answer\": llm_answer,\n",
        "                    \"right_answer\": right_answer,\n",
        "                    \"choices\": choices,\n",
        "                    \"extracted_answer\": extracted_answer \n",
        "                }\n",
        "\n",
        "                is_correct, verification = verify_llm_answer2(\n",
        "                    LLM_name=\"GPT4o\",\n",
        "                    prompt=prompt,\n",
        "                    llm_answer=extracted_answer,\n",
        "                    right_answer=right_answer,\n",
        "                    choices=choices,\n",
        "                    values=values\n",
        "                )\n",
        "\n",
        "                # Update the DataFrame with the verification result\n",
        "                df.at[index, is_correct_column] = is_correct\n",
        "                df.at[index, verif_column] = verification\n",
        "                df.at[index, extracted_answer_column] = extracted_answer if delimiter_exists else \"\"\n",
        "\n",
        "                errors_list.append(f\"{task_id} - {model_name}\")\n",
        "\n",
        "    # Print the total number of errors and the task_id - model combinations\n",
        "    total_errors = len(errors_list)\n",
        "    print(f\"\\nTotal number of questions with errors or empty answers: {total_errors}\")\n",
        "    print(\"Task ID - Model combinations with errors or empty answers:\")\n",
        "    for error in errors_list:\n",
        "        print(error)\n",
        "\n",
        "    # Save the modified DataFrame to a new Excel file\n",
        "    df.to_excel(output_excel_file_path, index=False)\n",
        "    print(f\"\\nModified Excel file saved to {output_excel_file_path}\")\n",
        "\n",
        "    # Check for errors after processing\n",
        "    updated_errors_list = []\n",
        "    for col in answer_columns:\n",
        "        model_name = col.replace('_answer', '')\n",
        "        for index, value in df[col].items():\n",
        "            if pd.isna(value) or 'Error' in str(value) or 'error' in str(value):\n",
        "                task_id = df.at[index, 'task_id']\n",
        "                updated_errors_list.append(f\"{task_id} - {model_name}\")\n",
        "\n",
        "    # Print final error count and task_id-model combinations\n",
        "    total_updated_errors = len(updated_errors_list)\n",
        "    print(f\"\\nTotal number of questions still with errors or empty answers after processing: {total_updated_errors}\")\n",
        "    print(\"Task ID - Model combinations still with errors or empty answers:\")\n",
        "    for updated_error in updated_errors_list:\n",
        "        print(updated_error)\n",
        "\n",
        "# Example usage\n",
        "# excel_file_path = 'path_to_your_excel_file.xlsx'\n",
        "# output_excel_file_path = 'path_to_save_updated_excel_file.xlsx'\n",
        "# relaunch_errors(excel_file_path, output_excel_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
