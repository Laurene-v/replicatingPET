{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9i_M5yQ5BX8"
      },
      "source": [
        "This code segment creates new useable benchmarks based on the raw initial benchmarks.\n",
        "After applying this code, the 5 benchmarks have the same structure and have been filtered to select questions with question marks, and the punctuation typos have been fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CVs8Duk4FIu"
      },
      "source": [
        "## Imports and installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hIOiLLUZLDm6"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import regex as re\n",
        "import shutil\n",
        "import itertools\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH_9cDhsS1zz"
      },
      "source": [
        "# Restructuring benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5qCcj544FIx"
      },
      "source": [
        "## Loading and saving data files from and into JSON or Excel files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_5TcivV4FIx"
      },
      "source": [
        "Functions:\n",
        "- load_data(path) : Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "    - Args:\n",
        "        path (str): the file path\n",
        "    - Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "- save_data(data, path): Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "    - Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "    - Returns:\n",
        "        bool: True if data is successfully saved, False otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z9fqnEFL4FIx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "\n",
        "    Args:\n",
        "        path (str): the file path\n",
        "\n",
        "    Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "    \"\"\"\n",
        "    data = None\n",
        "\n",
        "    if path.endswith('.xlsx'):\n",
        "        try:\n",
        "            data = pd.read_excel(path).to_dict(orient='records')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from Excel file: {str(e)}\")\n",
        "\n",
        "    elif path.endswith('.json'):\n",
        "        try:\n",
        "            with open(path, 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from JSON file: {str(e)}\")\n",
        "    else:\n",
        "        print(\"Unsupported file format. Please provide an Excel (.xlsx) or JSON (.json) file.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data(data, path):\n",
        "    \"\"\"\n",
        "    Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "\n",
        "    Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "\n",
        "    Returns:\n",
        "        bool: True if data is successfully saved, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if path.endswith('.json'):\n",
        "            if isinstance(data, dict):\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data, json_file, indent=2)\n",
        "            elif isinstance(data, pd.DataFrame):\n",
        "                data_dict = data.to_dict(orient='list')\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data_dict, json_file, indent=2)\n",
        "\n",
        "        elif path.endswith('.xlsx'):\n",
        "            if isinstance(data, dict):\n",
        "                data = pd.DataFrame.from_dict(data)\n",
        "            data.to_excel(path, index=False)\n",
        "        else:\n",
        "            print(\"Unsupported file format. Please provide a JSON (.json) or Excel (.xlsx) file path.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F--0v8J4FIy"
      },
      "source": [
        "## From the original benchmark to a common json structure\n",
        "\n",
        "structure of \"id\": {\n",
        "    \"question\": \"...?\",\n",
        "    \"choices\": null, OR \"choices\": [\n",
        "                        \"choice 1\",\n",
        "                        \"choice 2\",\n",
        "                        ...\n",
        "                        \"choice X\"\n",
        "                        ],\n",
        "    \"right_answer\": \"answer\"}\n",
        "\n",
        "\n",
        "Functions:\n",
        "- reformat_benchmark(input_path, output_path, structure_mapping):\n",
        "- reformat_benchmark_numglue (input_path, output_reformated_path):\n",
        "- reformat_xlsx(excel_file_path, reformated_path): for the CRT benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZmxrnvYM4FIy",
        "outputId": "759feb36-faa9-469f-c7eb-c2d0d6762ccb"
      },
      "outputs": [],
      "source": [
        "def reformat_benchmark(input_path, output_path, structure_mapping):\n",
        "    \"\"\" Reformats the benchmark from the original structure (input_path) to a uniformized json structure (output_path)\n",
        "    The uniformized structure (defined by the structure_mapping) is the following :\n",
        "    \"id\": {\n",
        "        \"question\": \"...?\",\n",
        "        \"choices\": null, OR \"choices\": [\n",
        "                        \"choice 1\",\n",
        "                        \"choice 2\",\n",
        "                        ...\n",
        "                        \"choice X\",\n",
        "                        ],\n",
        "        \"right_answer\": \"answer\"\n",
        "    }\n",
        "\n",
        "    This is the structure that will be used for the rest of the pipeline, so each benchmark should be reformatted to this structure for the following steps.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): path of the original benchmark\n",
        "        output_path (str): path where the uniformized json should be written in\n",
        "        structure_mapping (dict): mapping of the original structure to the uniformized structure\n",
        "    Returns:\n",
        "        None\n",
        "    \n",
        "    \"\"\"\n",
        "    data = load_data(input_path)\n",
        "\n",
        "    reformatted_data = []\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            new_instance = {}\n",
        "            for field, mapping in structure_mapping.items():\n",
        "                if mapping is not None:\n",
        "                    new_instance[field] = mapping(item)\n",
        "            reformatted_data.append(new_instance)\n",
        "    elif isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            new_instance = {}\n",
        "            for field, mapping in structure_mapping.items():\n",
        "                if mapping is not None:\n",
        "                    new_instance[field] = mapping(value)\n",
        "            reformatted_data[key] = new_instance\n",
        "\n",
        "    df = pd.DataFrame.from_dict(reformatted_data)\n",
        "    df.to_json(output_path, orient=\"index\", indent=2)\n",
        "\n",
        "\n",
        "# Examples of structure_mapping for the reformat_benchmark function:\n",
        "\n",
        "scienceQA_structure = {\n",
        "    \"question\": lambda value: value[\"question\"],\n",
        "    \"choices\": lambda value: value[\"choices\"],\n",
        "    \"right_answer\": lambda value: str(value[\"answer\"])\n",
        "}\n",
        "\n",
        "commonsenseQA_structure = {\n",
        "    \"question\": lambda value: value[\"question\"][\"stem\"],\n",
        "    \"choices\": lambda value: [choice[\"text\"] for choice in value[\"question\"][\"choices\"]],\n",
        "    \"right_answer\": lambda value: next(choice[\"text\"] for choice in value[\"question\"][\"choices\"] if choice[\"label\"] == value[\"answerKey\"])\n",
        "}\n",
        "\n",
        "strategyQA_structure = {\n",
        "    \"question\": lambda value: f\"Question: {value['question']}\\nFacts: {', '.join(fact.rstrip('.') for fact in value['facts'])}.\",\n",
        "    \"choices\": lambda value: None,\n",
        "    \"right_answer\": lambda value: str(value[\"answer\"])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "esKTiMN34FIy"
      },
      "outputs": [],
      "source": [
        "# reformatting functions specific to some benchmarks\n",
        "def reformat_benchmark_numglue (input_path, output_reformated_path):\n",
        "\n",
        "    new_data = {}\n",
        "    questions_count = 1\n",
        "\n",
        "    with open(input_path, 'r') as json_file:\n",
        "\n",
        "        for line in json_file:\n",
        "            data = json.loads(line)\n",
        "\n",
        "            if data[\"type\"] == \"Type_1\":\n",
        "\n",
        "                new_instance = {\n",
        "                    \"question\": data[\"question\"],\n",
        "                    \"choices\": None,\n",
        "                    \"right_answer\": str(data[\"answer\"])\n",
        "                }\n",
        "\n",
        "                new_data[questions_count] = new_instance\n",
        "                questions_count += 1\n",
        "\n",
        "    save_data(new_data, output_reformated_path)\n",
        "\n",
        "\n",
        "\n",
        "def reformat_CRT(excel_file_path, reformated_path):\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "\n",
        "    json_data = {}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        question_id = str(index + 1)\n",
        "        question_data = {\n",
        "            \"question\": row[\"tasks\"],\n",
        "            \"choices\": None,\n",
        "            \"right_answer\": row[\"correct_number\"]\n",
        "        }\n",
        "\n",
        "        json_data[question_id] = question_data\n",
        "\n",
        "    save_data(json_data, reformated_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lFmVQl-4FIy"
      },
      "source": [
        "## File processing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iSC0cml4FIy"
      },
      "source": [
        "Merge, update question ids, shuffle choices, filter duplicate questions, select X random questions\n",
        "\n",
        "Functions :\n",
        "- merge_json_files(folder_path, output_path): Reads all of the files in folder_path, merges them into a json file in output_path\n",
        "    - Args:\n",
        "        - folder_path (str): path of the folder in which all of the files will be merged\n",
        "        - output_path (str): path where the output should be written\n",
        "        \n",
        "<br>\n",
        "\n",
        "- update_quest_ids(file_path, updated_file_path):\n",
        "    \"\"\"Opens the json file at file_path, updates all the question IDs for them to start from 1 and increment by 1 for each question\n",
        "    Position :\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        updated_file_path (str): path where the json file with updated IDs should be written in\n",
        "<br>\n",
        "\n",
        "- shuffle_choices(file_path, updated_file_path):\n",
        "    \"\"\"Opens the json file at file_path, shuffles the choices so that the order is random\n",
        "    Position :\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        updated_file_path (str): path where the json file with shuffled choices should be written in\n",
        "\n",
        "\n",
        "- filter_duplicate_questions(input_file, output_file):\n",
        "\n",
        "- random_selection (file_path, number_selected, new_file_path):\n",
        "    \"\"\"Opens the json file at file_path, selects number_selected questions from this file, and writes them in the same json format in new_file_path\n",
        "    Position:\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        number_selected (int): number of questions to keep. Should be smaller that the total number of questions included in the file, otherwise all questions will be selected\n",
        "        new_file_path (str): path where the json file with updated IDs should be written in\n",
        "    \"\"\"\n",
        "\n",
        "- fix_regex_questions (input_path, output_reformated_path): corrects some easy punctuation issues\n",
        "\n",
        "- add_question_id(json_file_path): adds a question_id with the name of the benchmark included\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "puGSUHBGSo52"
      },
      "outputs": [],
      "source": [
        "def merge_json_files(folder_path, output_path):\n",
        "    \"\"\"Reads all of the files in folder_path, merges them into a json file in output_path\n",
        "    Position in pipeline: Before step 1 in Benchmark processing, only for benchmarks that are divided in different json files\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): path of the folder in which all of the files will be merged\n",
        "        output_path (str): path where the output should be written\n",
        "    \"\"\"\n",
        "\n",
        "    merged_data = {}\n",
        "    num_files = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.json'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            try:\n",
        "                data = load_data(file_path)\n",
        "\n",
        "                prefixed_data = {f\"{filename}_{key}\": value for key, value in data.items()}\n",
        "                merged_data.update(prefixed_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    save_data(merged_data, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "34qUT9yHbmq-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Selection of random questions from the datasets\n",
        "\n",
        "def random_selection (file_path, number_selected, new_file_path):\n",
        "    \"\"\"Opens the json file at file_path, selects number_selected questions from this file, and writes them in the same json format in new_file_path\n",
        "    Position:\n",
        "\n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        number_selected (int): number of questions to keep. Should be smaller that the total number of questions included in the file, otherwise all questions will be selected\n",
        "        new_file_path (str): path where the json file with updated IDs should be written in\n",
        "    \"\"\"\n",
        "\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    instance_ids = list(data.keys())\n",
        "    random.shuffle(instance_ids)\n",
        "\n",
        "    # Number of questions to be selected\n",
        "    if number_selected > len(instance_ids):\n",
        "        selected_ids = instance_ids\n",
        "        print(\"Number in argument bigger than the number of questions,\", len(instance_ids), \" questions selected\")\n",
        "    else:\n",
        "        selected_ids = instance_ids[:number_selected]\n",
        "\n",
        "    selected_data = {}\n",
        "    for instance_id in selected_ids:\n",
        "        selected_data[instance_id] = data[instance_id]\n",
        "\n",
        "    save_data(selected_data, new_file_path)\n",
        "\n",
        "def filter_duplicate_questions(input_file, output_file):\n",
        "    seen_questions = set()\n",
        "    filtered_data = []\n",
        "\n",
        "    with open(input_file, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    for instance_id, instance in data.items():\n",
        "        question = instance.get(\"question\", \"\")\n",
        "        if question not in seen_questions:\n",
        "            filtered_data.append((instance_id, instance))\n",
        "            seen_questions.add(question)\n",
        "        #section to comment out\n",
        "        else:\n",
        "            print (\"Question not taken : \", question)\n",
        "\n",
        "    filtered_data_dict = dict(filtered_data)\n",
        "\n",
        "    with open(output_file, 'w') as output_json:\n",
        "        json.dump(filtered_data_dict, output_json, indent=2)\n",
        "        \n",
        "def shuffle_choices(file_path, updated_file_path):\n",
        "    \"\"\"Opens the json file at file_path, shuffles the choices so that the order is random\n",
        "    Position :\n",
        "\n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        updated_file_path (str): path where the json file with shuffled choices should be written in\n",
        "    \"\"\"\n",
        "\n",
        "    data = load_data(file_path)\n",
        "    new_data = {}\n",
        "\n",
        "    for instance_id, instance_data in data.items():\n",
        "        if \"choices\" in instance_data and instance_data[\"choices\"]!= None:\n",
        "            random.shuffle(instance_data[\"choices\"])\n",
        "        new_instance = instance_data.copy()\n",
        "        new_data[instance_id] = new_instance\n",
        "\n",
        "    save_data(new_data, updated_file_path)\n",
        "    \n",
        "def update_quest_ids(file_path, updated_file_path):\n",
        "    \"\"\"Opens the json file at file_path, updates all the question IDs for them to start from 1 and increment by 1 for each question\n",
        "    Position :\n",
        "\n",
        "    Args:\n",
        "        file_path (str): path of the json file to be updated\n",
        "        updated_file_path (str): path where the json file with updated IDs should be written in\n",
        "    \"\"\"\n",
        "\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    new_id = 1\n",
        "\n",
        "    new_data = {}\n",
        "\n",
        "    for instance_id, instance_data in data.items():\n",
        "        new_instance = instance_data.copy()\n",
        "        #new_instance[\"id\"] = str(new_id)\n",
        "        new_data[str(new_id)] = new_instance\n",
        "        new_id += 1\n",
        "\n",
        "    save_data(new_data, updated_file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "U5XS05Hs4FIz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fix_regex_questions (input_path, output_reformated_path):\n",
        "\n",
        "    new_data = {}\n",
        "    questions_count = 1\n",
        "\n",
        "    with open(input_path, 'r') as json_file:\n",
        "\n",
        "        data = json.load(json_file)\n",
        "        for instance_id, instance in data.items():\n",
        "            old_question = instance.get(\"question\", \"\")\n",
        "\n",
        "            # fix spacing and punctuation issues\n",
        "            # careful : has a specific optimised order\n",
        "            question = \"\"\n",
        "            question_upd = \"update\"\n",
        "\n",
        "            while question != question_upd: #repeat in case new patterns have been created by the previous set of .replace\n",
        "                question_upd = question\n",
        "                question = old_question.replace('  ', ' ')\n",
        "                question = question.replace('\\n\\n.', '\\n\\n')\n",
        "                question = question.replace(': , ', ': ')\n",
        "                question = question.replace(' ,', ',')\n",
        "                question = question.replace(' .', '.')\n",
        "                question = question.replace('..', '.')\n",
        "                question = question.replace('?.', '?')\n",
        "                question = question.replace(':,', ': ')\n",
        "\n",
        "            # only select questions with interrogation points\n",
        "            if \"?\" in question:\n",
        "\n",
        "                new_instance = {\n",
        "                        \"question\": question,\n",
        "                        \"original_question\": instance.get(\"original_question\"),\n",
        "                        \"choices\": instance.get(\"choices\"),\n",
        "                        \"right_answer\": instance.get(\"right_answer\"),\n",
        "                        \"benchmark\": \"\"\n",
        "                }\n",
        "\n",
        "                new_data[questions_count] = new_instance\n",
        "\n",
        "                questions_count += 1\n",
        "            \"\"\"else:\n",
        "                print(\"question with no ? : \", question)\"\"\"\n",
        "\n",
        "    with open(output_reformated_path, 'w') as new_json_file:\n",
        "        json.dump(new_data, new_json_file, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "iemfgVho4FIz",
        "outputId": "c317d141-a801-4c1d-d8a7-29558d94681c"
      },
      "outputs": [],
      "source": [
        "# adding the question id as part of the attributes\n",
        "def add_question_id(json_file_path, benchmark = None):\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for question_id, question_data in data.items():\n",
        "        benchmark_name = benchmark\n",
        "        if benchmark_name == None and \"benchmark\" in question_data:\n",
        "            benchmark_name = question_data[\"benchmark\"]\n",
        "        else:\n",
        "            benchmark_name = json_file_path.split(\"/\")[-1].split(\"_\")[0]\n",
        "\n",
        "        question_data[\"benchmark\"] = benchmark_name\n",
        "        question_data[\"question_id\"] = benchmark_name + \"_\" + question_id\n",
        "\n",
        "    with open(json_file_path, 'w') as file:\n",
        "        json.dump(data, file, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHEbtEQu4FI0"
      },
      "source": [
        "## From the common structure to a smaller useable benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRqduNxk4FI0"
      },
      "source": [
        "The new benchmark has number_selected questions, choices are shuffled and included in the question, the questions are cleaned (no double spaces, etc...)\n",
        "\n",
        "Functions:\n",
        "- cleaned_final_benchmark(file_path, new_file_path, number_selected = 175):\n",
        "- add_choices_in_question(benchmark_path, new_benchmark_path):\n",
        "    \"\"\"Adds \"the choices are ...\" to the benchmark questions\n",
        "\n",
        "    Args:\n",
        "        benchmark_path (str): path of the benchmark to be updated\n",
        "        new_benchmark_path (str): path of the updated benchmark with choices in the question\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "T14GBSd84FI0"
      },
      "outputs": [],
      "source": [
        "def check_answers_regex(answer_a, answer_b):\n",
        "    \"\"\"Checks if answer_a equals answer_be after ignoring differences in capitalization or symbols\n",
        "\n",
        "    Args:\n",
        "        answer_a (str): an answer\n",
        "        answer_b (str): another answer to be compared to answer_a\n",
        "\n",
        "    Returns:\n",
        "        bool: True if both answers are the same, False otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure answers are strings\n",
        "    answer_a = str(answer_a) if answer_a is not None else \"\"\n",
        "    answer_b = str(answer_b) if answer_b is not None else \"\"\n",
        "\n",
        "    # Remove common variations and convert to lowercase\n",
        "    normalized_a = re.sub(r'(?i)answer|the solution is|[^a-zA-Z0-9]', '', answer_a).lower()\n",
        "    normalized_b = re.sub(r'(?i)answer|the solution is|[^a-zA-Z0-9]', '', answer_b).lower()\n",
        "\n",
        "    # Compare the normalized answers\n",
        "    return normalized_a == normalized_b\n",
        "\n",
        "\n",
        "def add_choices_in_question(benchmark_path, new_benchmark_path, lettered_choices = True):\n",
        "    \"\"\"Adds \"the choices are ...\" to the benchmark questions\n",
        "\n",
        "    Args:\n",
        "        benchmark_path (str): path of the benchmark to be updated\n",
        "        new_benchmark_path (str): path of the updated benchmark with choices in the question\n",
        "    \"\"\"\n",
        "\n",
        "    benchmark_with_intro = {}\n",
        "    data = load_data(benchmark_path)\n",
        "\n",
        "    for key, value in data.items():\n",
        "        # if open question:\n",
        "        old_question = str(value.get(\"question\", \"\")).strip()\n",
        "        new_question = str(value.get(\"question\", \"\")).strip()\n",
        "\n",
        "        # if boolean question:\n",
        "        if check_answers_regex(value[\"right_answer\"], \"True\") or check_answers_regex(value[\"right_answer\"], \"False\"):\n",
        "            new_question += \". \\nAnswer by 'True' or 'False'. \"\n",
        "\n",
        "        # if multiple choice question:\n",
        "        elif value[\"choices\"]!= None:\n",
        "            if lettered_choices:\n",
        "                choice_dict = generate_choice_dict(value[\"choices\"])\n",
        "                new_question += \"\\n\\nThe choices are the following: \" + json.dumps(choice_dict) + \"\\nOnly select the letter associated with the choice. \\n Your response should be a single letter, with no other text whatsoever.\"\n",
        "\n",
        "        new_instance = {\n",
        "                \"original_question\": old_question,\n",
        "                \"question\": new_question,\n",
        "                \"choices\": value[\"choices\"],\n",
        "                \"right_answer\": value[\"right_answer\"]\n",
        "            }\n",
        "\n",
        "        benchmark_with_intro[key] = new_instance\n",
        "\n",
        "    save_data(benchmark_with_intro, new_benchmark_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "-29tHjlt4FI0"
      },
      "outputs": [],
      "source": [
        "#Creation of a smaller benchmark from the common-structure benchmark\n",
        "\n",
        "def cleaned_final_benchmark(file_path, new_file_path, number_selected = 150, name_benchmark=\"no_name\", lettered_choices = False, temp_path = \"./temp_file_replicatingPET.json\"):\n",
        "\n",
        "    # --- Restructuring to a common format --- #\n",
        "    # input: the benchmark already under a specific format, such as:\n",
        "    \"\"\" \"quest_nb_X\": {\n",
        "        \"question\": \"This is a very interesting question, don't you think?\",\n",
        "        \"choices\": [\"True\", \"yes\", \"of course\"],\n",
        "        \"right_answer\": \"of course\"\n",
        "    }, \"\"\"\n",
        "\n",
        "\n",
        "    # output: the json file with indented integers as question ids,\n",
        "    # a \"original question\" element which will not change (to keep a trace of the original question at all stages),\n",
        "    # and a \"question_id\" element associated with the original_question of the benchmark\n",
        "    # Example :\n",
        "    \"\"\" \"X\": {\n",
        "        \"question\": \"This is a very interesting question, don't you think?\",\n",
        "        \"original_question\": \"This is a very interesting question, don't you think?\",\n",
        "        \"choices\": [\"True\", \"yes\", \"of course\"],\n",
        "        \"right_answer\": \"of course\",\n",
        "        \"question_id\": \"nameofbenchmark_1\"\n",
        "    },\"\"\"\n",
        "    # correct number of questions, no duplicates, question filtered (no punctuation issues like double spaces, all questions have an interrogation point)\n",
        "    # for questions with choices, a part is added in the question: \"the choices are the following : \" + choices\n",
        "   \n",
        "\n",
        "    random_selection(file_path, round(number_selected*1.10), temp_path)\n",
        "    filter_duplicate_questions(temp_path, temp_path)\n",
        "    shuffle_choices(temp_path, temp_path)\n",
        "    add_choices_in_question(temp_path, temp_path, lettered_choices)\n",
        "    fix_regex_questions (temp_path, temp_path) \n",
        "    random_selection(temp_path, number_selected, temp_path)\n",
        "    update_quest_ids(temp_path, new_file_path)\n",
        "    add_question_id(new_file_path, name_benchmark)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Mu7oPWJa0t"
      },
      "source": [
        "# Complete a benchmark :\n",
        "\n",
        "If we use this function on a benchmark that has fewer questions than the number in argument, it completes it to reach the correct number of questions.\n",
        "If we use it on a benchmark with a higher number of questions, it only selects the requested number of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fye9Pc2v2yz_"
      },
      "outputs": [],
      "source": [
        "def complete_benchmark(full_structured_benchmark, small_benchmark_path, benchmark_name, newJSON_path, added_quest_json_path, number_of_questions=150, temp_path = \"./simplified_folder/temporary_json.json\"):\n",
        "\n",
        "    # fix questions from the restructured benchmark\n",
        "    filter_duplicate_questions(full_structured_benchmark, temp_path)\n",
        "    shuffle_choices(temp_path, temp_path)\n",
        "    update_quest_ids(temp_path, temp_path)\n",
        "    add_choices_in_question(temp_path, temp_path)\n",
        "    fix_regex_questions (temp_path, temp_path)\n",
        "\n",
        "    with open(temp_path, 'r') as file1, open(small_benchmark_path, 'r') as file2:\n",
        "        json1_data = json.load(file1)\n",
        "        json2_data = json.load(file2)\n",
        "\n",
        "    # Extract questions from the benchmark to complete\n",
        "    questions_from_json2 = list(json2_data.values())\n",
        "    initial_length = len(questions_from_json2)\n",
        "    nb_added_quest = 0\n",
        "\n",
        "    # Check if it has more questions than required\n",
        "    if len(questions_from_json2) > number_of_questions:\n",
        "        print(\"The benchmark already has more questions than required.\")\n",
        "        selected_questions = questions_from_json2[:number_of_questions]\n",
        "        print(\"Benchmark shortened to \", number_of_questions, \" questions.\")\n",
        "    else:\n",
        "        selected_questions = questions_from_json2\n",
        "\n",
        "    # Randomly shuffle questions from the full benchmark\n",
        "    questions_from_json1 = list(json1_data.values())\n",
        "    random.shuffle(questions_from_json1)\n",
        "\n",
        "    # Add questions from the full benchmark to the small benchmark\n",
        "    while len(selected_questions) < number_of_questions and questions_from_json1:\n",
        "        next_question = questions_from_json1.pop()\n",
        "        # Check if the question is not a duplicate in selected_questions\n",
        "        if all(next_question['question'] != q['question'] for q in selected_questions):\n",
        "            selected_questions.append(next_question)\n",
        "            nb_added_quest +=1\n",
        "\n",
        "    # Create a new JSON with the selected questions\n",
        "    new_json_data = {str(i + 1): question for i, question in enumerate(selected_questions)}\n",
        "\n",
        "    # Write the new JSON data to a file\n",
        "    with open(newJSON_path, 'w') as new_file:\n",
        "        json.dump(new_json_data, new_file, indent=2)\n",
        "\n",
        "    update_quest_ids(newJSON_path, newJSON_path)\n",
        "    add_question_id(newJSON_path, benchmark_name)\n",
        "\n",
        "    #print(\"newJSON_path: \", newJSON_path, \"added quest :\", nb_added_quest)\n",
        "\n",
        "    try:\n",
        "        with open(newJSON_path, 'r') as json_file:\n",
        "            newdata = json.load(json_file)\n",
        "        #print(\"newdata: \", newdata)\n",
        "\n",
        "        if isinstance(newdata, dict):\n",
        "            newdata = list(newdata.values())  # Convert the dictionary to a list of values\n",
        "\n",
        "        added_questions = {str(i + 1): item for i, item in enumerate(newdata[initial_length:])}\n",
        "\n",
        "        with open(added_quest_json_path, 'w') as output_file:\n",
        "            json.dump(added_questions, output_file, indent=2)\n",
        "    except Exception as e:\n",
        "        print(\"Error creating the added_questions file:\", str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 1 Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with StrategyQA:\n",
        "\n",
        "folder_path = \"YOUR_FOLDER_PATH\"\n",
        "\n",
        "unedited_benchmark_path = folder_path + \"strategyqa.json\"\n",
        "reformated_benchmark_path = folder_path + \"strategyqa_reformated.json\"\n",
        "cleaned_benchmark_path = folder_path + \"strategyqa_cleaned.json\"\n",
        "temporary_path = folder_path + \"temporary_strategyqa.json\"\n",
        "\n",
        "strategyQA_structure = {\n",
        "    \"question\": lambda value: f\"Question: {value['question']}\\nFacts: {', '.join(fact.rstrip('.') for fact in value['facts'])}.\",\n",
        "    \"choices\": lambda value: None,\n",
        "    \"right_answer\": lambda value: str(value[\"answer\"])\n",
        "}\n",
        "\n",
        "reformat_benchmark(unedited_benchmark_path, reformated_benchmark_path, strategyQA_structure)\n",
        "cleaned_final_benchmark(reformated_benchmark_path, cleaned_benchmark_path, 150, \"strategyqa\", lettered_choices = False, temp_path = temporary_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
