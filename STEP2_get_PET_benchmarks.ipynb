{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRj0HjWc-tjP"
      },
      "source": [
        "This code segment adapts the selected useable benchmarks and modifies the prompting, either by adding a prefix or a suffix to the tasks, or by asking an LLM to rewrite the prompts given a specific instruction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMbFVaA3Rs1k"
      },
      "source": [
        "## Imports and installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hIOiLLUZLDm6"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "import logging\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflzMCzERs1t"
      },
      "source": [
        "## Loading and saving data files from and into JSON or Excel files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QeAPOehRs1u"
      },
      "source": [
        "Functions:\n",
        "- load_data(path) : Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "    - Args:\n",
        "        path (str): the file path\n",
        "    - Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "- save_data(data, path): Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "    - Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "    - Returns:\n",
        "        bool: True if data is successfully saved, False otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hB3Ppn3PRs1u"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "\n",
        "    Args:\n",
        "        path (str): the file path\n",
        "\n",
        "    Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "    \"\"\"\n",
        "    data = None\n",
        "\n",
        "    if path.endswith('.xlsx'):\n",
        "        try:\n",
        "            data = pd.read_excel(path).to_dict(orient='records')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from Excel file: {str(e)}\")\n",
        "\n",
        "    elif path.endswith('.json'):\n",
        "        try:\n",
        "            with open(path, 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from JSON file: {str(e)}\")\n",
        "    else:\n",
        "        print(\"Unsupported file format. Please provide an Excel (.xlsx) or JSON (.json) file.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data(data, path):\n",
        "    \"\"\"\n",
        "    Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "\n",
        "    Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "\n",
        "    Returns:\n",
        "        bool: True if data is successfully saved, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if path.endswith('.json'):\n",
        "            if isinstance(data, dict):\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data, json_file, indent=2)\n",
        "            elif isinstance(data, pd.DataFrame):\n",
        "                data_dict = data.to_dict(orient='list')\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data_dict, json_file, indent=2)\n",
        "\n",
        "\n",
        "        elif path.endswith('.xlsx'):\n",
        "\n",
        "            if isinstance(data, dict):\n",
        "                # Convert dictionary to DataFrame if it's a dictionary\n",
        "                data = pd.DataFrame.from_dict(data)\n",
        "            data.to_excel(path, index=False)\n",
        "        else:\n",
        "            print(\"Unsupported file format. Please provide a JSON (.json) or Excel (.xlsx) file path.\")\n",
        "            return False\n",
        "\n",
        "        #print(f\"Data saved to {path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfMUfjj8b6Yt"
      },
      "source": [
        "## Adding prompt engineering pre- and suffixes in the benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section modifies benchmarks with prompt engineering techniques based on adding some instructions or description before and/or after the question, such as zero-shot chain-of-thought prompting (\"Answer step-by-step\") or ExpertPrompting (\"You are an expert in ...\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6h-qBLcRs2G"
      },
      "source": [
        "Functions :\n",
        "- questions_with_intro(intro_prompt_dict, benchmark_path, benchmark_with_intro_path):\n",
        "    \"\"\"Adds introduction prompts to the benchmark questions\n",
        "\n",
        "    Args:\n",
        "        intro_prompt_dict (dict): dictionary containing all introduction prompts.\n",
        "        Each instance should be in the following format: {\"FirstName\" : [\"Introduction prompt\", \" Additional prompt added at the end of the question\", \"education level: either 'educated', 'uneducated', or 'neutral' \"]}\n",
        "        benchmark_path (str): path of the benchmark to be updated\n",
        "        benchmark_with_intro_path (str): path of the updated benchmark with introduction prompts\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yxSQddFAdBMx"
      },
      "outputs": [],
      "source": [
        "def questions_with_intro( benchmark_path, folder_benchmark_with_intros, benchmark_name, intro_prompt_dict = {}):\n",
        "    \"\"\"Adds introduction prompts to the benchmark questions\n",
        "\n",
        "    Args:\n",
        "        intro_prompt_dict (dict): dictionary containing all introduction prompts.\n",
        "        Each instance should be in the following format: {\"FirstName\" : [\"Introduction prompt\", \" Additional prompt added at the end of the question\", \"prompt_characteristic: either 'educated', 'uneducated', or 'neutral' \"]}\n",
        "        benchmark_path (str): path of the benchmark to be updated\n",
        "        benchmark_with_intro_path (str): path of the updated benchmark with introduction prompts\n",
        "    \"\"\"\n",
        "    #creating a folder to put in the rewritten benchmarks, if it does not already exist\n",
        "    if not os.path.exists(folder_benchmark_with_intros):\n",
        "        os.makedirs(folder_benchmark_with_intros)\n",
        "\n",
        "    for persona, promptlist in intro_prompt_dict.items():\n",
        "        benchmark_with_intro = {}\n",
        "\n",
        "        suffix = promptlist[1]\n",
        "        prompt = promptlist[0]\n",
        "        #print(promptlist, suffix, prompt)\n",
        "\n",
        "        data = load_data(benchmark_path)\n",
        "\n",
        "        for key, value in data.items():\n",
        "            new_question = prompt + \"\\n\" + \"Answer this question: \" + \"\\n\" + value[\"question\"] + \"\\n\" + suffix\n",
        "\n",
        "            new_instance = {\n",
        "                    \"question\": new_question,\n",
        "                    \"original_question\": value[\"original_question\"],\n",
        "                    \"choices\": value[\"choices\"],\n",
        "                    \"right_answer\": value[\"right_answer\"],\n",
        "                    \"benchmark\": value[\"benchmark\"],\n",
        "                    \"question_id\": value[\"question_id\"],\n",
        "                    \"persona\": persona,\n",
        "                    \"prompt_characteristic\": promptlist[2]\n",
        "                }\n",
        "\n",
        "            benchmark_with_intro[key] = new_instance\n",
        "\n",
        "        # Defining a name for the created files\n",
        "        # careful: some later functions are based on the systems of underscores here to retrieve values,\n",
        "        # they will not work if the name is not under the format benchmarkname_persona_usercharacteristic_llm.json\n",
        "        # (no extra underscore should be in the names)\n",
        "\n",
        "        personalised_benchmark_path = folder_benchmark_with_intros + benchmark_name + \"_\" + persona +\"_\" + promptlist[2] + \".json\"\n",
        "\n",
        "\n",
        "        save_data(benchmark_with_intro, personalised_benchmark_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rephrasing the benchmarks following a specific prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section modifies benchmarks with prompt engineering techniques based on asking LLMs to rephrase the question.\n",
        "\n",
        "Functions :\n",
        "- create_response_GPT_base(question_prompt, basemodel, api_key=OPENAI_API_KEY):\n",
        "    Function for getting responses from GPT models.\n",
        "\n",
        "    Args:\n",
        "        question_prompt (str): The question prompt\n",
        "        basemodel (str): The model to use (e.g., \"gpt-4o\")\n",
        "        api_key (str, optional): OpenAI API key. Defaults to OPENAI_KEY.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response\n",
        "    \n",
        "- rephrase_questions(additional_prompt, input_path, output_reformated_path, persona, LLM_function = create_response_GPT4o):\n",
        "    Rephrases all questions in a benchmark JSON file using a given LLM function,\n",
        "    adds persona and prompt_characteristic fields, and saves the modified data.\n",
        "\n",
        "    Args:\n",
        "        additional_prompt (str): Text prompt with the rephrasing instructions\n",
        "        input_path (str): Path to the input JSON benchmark file.\n",
        "        output_reformated_path (str): Path where the rephrased JSON file will be saved.\n",
        "        persona (str): Label describing the persona/prompt characteristic to associate with the questions.\n",
        "        LLM_function (callable, optional): Function that takes a string input and returns\n",
        "            a rephrased version of the question (default: create_response_GPT4o).\n",
        "\n",
        "    Output:\n",
        "        Saves a formatted JSON file with the same structure as\n",
        "        the input, but with rephrased questions and added persona metadata.\n",
        "    \n",
        "- rephrase_all_prompts(benchmark_name, benchmark_path, rephrasing_dict={}, folder_new_benchmark = \"./rephrased_benchmarks/\"):\n",
        "    Applies the rephrase_questions function on all prompts in rephrasing_dict\n",
        "\n",
        "- add_base(input_path, benchmarkname, folder_new_benchmark = \"./modified-benchmarks/\"):\n",
        "    Adds 'persona' and 'prompt_characteristic' = 'base' to each entry for the control dataset.\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if rephrasing benchmarks with LLMs: \n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "def create_response_GPT_base(question_prompt, basemodel, api_key=OPENAI_API_KEY):\n",
        "    \"\"\"Base function for getting responses from GPT models.\n",
        "\n",
        "    Args:\n",
        "        question_prompt (str): The question prompt\n",
        "        basemodel (str): The model to use (e.g., \"gpt-4o\")\n",
        "        api_key (str, optional): OpenAI API key. Defaults to OPENAI_KEY.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response\n",
        "    \"\"\"\n",
        "    logger.info(f\"--- create_response base : {basemodel} --- \")\n",
        "\n",
        "    response = None\n",
        "    retries = 0\n",
        "\n",
        "    while retries < 3:\n",
        "        try:\n",
        "            client = OpenAI(api_key=api_key)\n",
        "\n",
        "            resp = client.chat.completions.create(\n",
        "                model=basemodel,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                seed=1,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": question_prompt}\n",
        "                ]\n",
        "            )\n",
        "            print(\"tokens : \", resp.usage.total_tokens)\n",
        "            print(\"model used : \", resp.model)\n",
        "            response = resp.choices[0].message.content\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "            print(question_prompt)\n",
        "            time.sleep(0.100)\n",
        "            response = f\"Error occurred: {e}\"\n",
        "            retries += 1\n",
        "\n",
        "    return response\n",
        "\n",
        "def create_response_GPT4o(question_prompt, api_key=OPENAI_API_KEY):\n",
        "    return create_response_GPT_base(question_prompt, \"gpt-4o\", api_key)\n",
        "\n",
        "# create_response_GPT4o(\"what is 2+2?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nXEvSm4Rs3J"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rephrase_questions(additional_prompt, input_path, output_reformated_path, persona, LLM_function = create_response_GPT4o):\n",
        "    \"\"\"\n",
        "    Rephrases all questions in a benchmark JSON file using a given LLM function,\n",
        "    adds persona and prompt_characteristic fields, and saves the modified data.\n",
        "\n",
        "    Args:\n",
        "        additional_prompt (str): Text prompt with the rephrasing instructions\n",
        "        input_path (str): Path to the input JSON benchmark file.\n",
        "        output_reformated_path (str): Path where the rephrased JSON file will be saved.\n",
        "        persona (str): Label describing the persona/prompt characteristic to associate with the questions.\n",
        "        LLM_function (callable, optional): Function that takes a string input and returns\n",
        "            a rephrased version of the question (default: create_response_GPT4o).\n",
        "\n",
        "    Output:\n",
        "        Saves a formatted JSON file at `output_reformated_path` with the same structure as\n",
        "        the input, but with rephrased questions and added persona metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    new_data = {}\n",
        "    questions_count = 1\n",
        "\n",
        "    with open(input_path, 'r') as json_file:\n",
        "\n",
        "        data = json.load(json_file)\n",
        "        for instance_id, instance in data.items():\n",
        "            question = instance.get(\"question\", \"\")\n",
        "            question = LLM_function(additional_prompt + question)\n",
        "\n",
        "            new_instance = {\n",
        "                    \"question\": question,\n",
        "                    \"original_question\": instance.get(\"original_question\"),\n",
        "                    \"choices\": instance.get(\"choices\"),\n",
        "                    \"right_answer\" : instance.get(\"right_answer\"),\n",
        "                    \"question_id\" : instance.get(\"question_id\"),\n",
        "                    \"benchmark\" : instance.get(\"benchmark\"),\n",
        "                    \"persona\": persona,\n",
        "                    \"prompt_characteristic\": persona\n",
        "            }\n",
        "\n",
        "            new_data[questions_count] = new_instance\n",
        "\n",
        "            questions_count += 1\n",
        "\n",
        "    with open(output_reformated_path, 'w') as new_json_file:\n",
        "        json.dump(new_data, new_json_file, indent=2)\n",
        "\n",
        "def rephrase_all_prompts(benchmark_name, benchmark_path, rephrasing_dict={}, folder_new_benchmark = \"./rephrased_benchmarks/\"):\n",
        "\n",
        "    #creating a folder to put in the rewritten benchmarks, if it does not already exist\n",
        "    if not os.path.exists(folder_new_benchmark):\n",
        "        os.makedirs(folder_new_benchmark)\n",
        "\n",
        "    for key, value in rephrasing_dict.items():\n",
        "        print(\" Rephrasing \", key)\n",
        "        rephrased_path = folder_new_benchmark + benchmark_name + \"_\" + key + \".json\"\n",
        "        rephrase_questions(value, benchmark_path, rephrased_path, key)\n",
        "        print(\"---\")\n",
        "\n",
        "\n",
        "def add_base(input_path, benchmarkname, folder_new_benchmark = \"./modified-benchmarks/\"):\n",
        "    \"\"\"\n",
        "    Adds 'persona' and 'prompt_characteristic' = 'base' to each entry for the control dataset.\n",
        "    \"\"\"\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for instance in data.values():\n",
        "        instance[\"persona\"] = \"base\"\n",
        "        instance[\"prompt_characteristic\"] = \"base\"\n",
        "\n",
        "    os.makedirs(folder_new_benchmark, exist_ok=True)\n",
        "    base_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
        "    output_path = os.path.join(folder_new_benchmark, f\"{benchmarkname}_base_base.json\")\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✅ File saved to {output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Re-Reading Prompt Engineering technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function is specific to the Re-Reading prompt engineering technique as it is based on a different structure than many of the other prefix or suffix-based prompt engineering technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rereading_benchmark(input_path, output_path):\n",
        "\n",
        "    with open(input_path, 'r') as infile:\n",
        "        input_json = json.load(infile)\n",
        "    \n",
        "    updated_json = {}\n",
        "\n",
        "    for key, value in input_json.items():\n",
        "        original_question_text = value[\"original_question\"].replace(\" \\nAnswer by only giving the correct option.\", \"\")\n",
        "        \n",
        "        new_question = (\"Solve the task below. Importantly, write your final answer after \\\"####\\\".\\n\\n\" + original_question_text + \"\\nRead the question again: \" + original_question_text + \"\\n\")\n",
        "\n",
        "        # Update benchmark and question_id\n",
        "        benchmark_name = value[\"benchmark\"].split('-')[0]\n",
        "        question_id = value[\"question_id\"]\n",
        "        #benchmark_name = \"CRT\"\n",
        "        #question_id = f\"{benchmark_name}_{key}\"\n",
        "\n",
        "        updated_json[key] = {\n",
        "            \"question\": new_question,\n",
        "            \"original_question\": original_question_text,\n",
        "            \"right_answer\": value[\"right_answer\"],\n",
        "            \"prompt_characteristic\": \"rereading\",\n",
        "            \"persona\": \"rereading\",\n",
        "            \"benchmark\": benchmark_name,\n",
        "            \"question_id\": question_id\n",
        "        }\n",
        "\n",
        "        if \"choices\" in value:\n",
        "            updated_json[key][\"choices\"] = value[\"choices\"]\n",
        "\n",
        "    with open(output_path, 'w') as outfile:\n",
        "        json.dump(updated_json, outfile, indent=2)\n",
        "    print(f\"Updated JSON has been saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate all benchmark variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_modified_benchmarks (benchmarkname, benchmarkpath, bioprefix_dict, rephrasing_dict, newfolder):\n",
        "     \"\"\"\n",
        "    Generates a complete set of modified benchmark variants by sequentially applying\n",
        "    all transformation functions (intro addition, LLM-based rephrasing, and base version creation).\n",
        "\n",
        "    Specifically, this function:\n",
        "      1. Calls `questions_with_intro()` to prepend custom introductions defined\n",
        "         in `bioprefix_dict` to the benchmark questions, producing multiple prefixed versions.\n",
        "      2. Calls `rephrase_all_prompts()` to rephrase the same benchmark using GPT-based\n",
        "         prompts from `rephrasing_dict`, generating multiple rephrased variants.\n",
        "      3. Calls `add_base()` to create a baseline version of the benchmark with\n",
        "         neutral persona fields ('base') for reference or comparison.\n",
        "\n",
        "    Args:\n",
        "        benchmarkname (str): Name identifier for the benchmark (used in output filenames).\n",
        "        benchmarkpath (str): Path to the original benchmark JSON file.\n",
        "        bioprefix_dict (dict): Mapping of prefix names to introductory text snippets\n",
        "            to be prepended to each question.\n",
        "        rephrasing_dict (dict): Mapping of persona names to prompt rephrasing instructions\n",
        "            for GPT-based question generation.\n",
        "        newfolder (str): Output directory where all generated benchmark variants will be saved.\n",
        "\n",
        "    Output:\n",
        "        Creates and saves:\n",
        "          - One version per prefix in `bioprefix_dict`\n",
        "          - One version per persona in `rephrasing_dict`\n",
        "          - One baseline version with 'persona' and 'prompt_characteristic' set to 'base'\n",
        "        All files are stored in the specified `newfolder`.\n",
        "    \"\"\"\n",
        "\n",
        "    questions_with_intro(benchmarkpath, newfolder, benchmarkname, bioprefix_dict)\n",
        "    # all N prefixes from the dictionary added, creation of N new versions of the benchmark\n",
        "\n",
        "    rephrase_all_prompts(benchmarkname, benchmarkpath, rephrasing_dict, newfolder)\n",
        "    # used an LLM (GPT4) to rephrase the benchmarks according to all M prompts in the rephrasing dictionary, creation of M rephrased versions of the benchmark in the selected folder\n",
        "\n",
        "    add_base(benchmarkpath, benchmarkname, folder_rephrased_benchmark)\n",
        "    #shutil.copyfile(benchmarkpath, folder_rephrased_benchmark + benchmarkname +'_base_neutral.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2 Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with StrategyQA:\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "folder_path = \"YOUR_FOLDER_PATH\"\n",
        "folder_modified_benchmarks_step2 = folder_path + \"step2_modified-benchmarks/\"\n",
        "cleaned_benchmark_path = folder_path + \"strategyqa_cleaned_10questions.json\"\n",
        "\n",
        "# examples of dictionaries for the prompt engineering techniques\n",
        "bioprefix_dict = {\"educatedpersona\" : [\"Hello, my name is Dr. Mark Davis. I am a globally recognized cardiologist from Los Angeles, California. I completed my studies at Yale University where I was awarded my PhD in Cardiology. My research primarily investigates the role of lifestyle in heart diseases. During my free time, I like to cycle, play chess, and read biographies. I also devote time to mentor young doctors in community health centers.\", \" Please reply succintly\", \"educated\"],\n",
        "                     \"uneducatedpersona\" : [\"Hello, my name is Billy Thompson. Hailing from Phoenix, Arizona, I never had much of a taste for schoolin', never graduated past fifth grade. I follow my own curiosities. Fishing, working on my motorcycle, and listening to classic rock records are how I like to spend my days. I'm also very good with my hands, building different things from spare parts lying around. You'll always find me helping folks fix things in the neighborhood.\", \" Reply with few words\", \"uneducated\"],\n",
        "                     }\n",
        "rephrasing_dict = {\"promptvariation\" : \"\"\"Generate a variation of the following instruction while keeping the semantic meaning. \\n\n",
        "Input : \n",
        "\"\"\"}\n",
        "\n",
        "create_modified_benchmarks(\"strategyQA\", cleaned_benchmark_path, bioprefix_dict, rephrasing_dict, folder_new_benchmarks)\n",
        "rereading_benchmark(cleaned_benchmark_path, folder_new_benchmarks + \"strategyQA_rereading_rereading.json\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
