{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "import logging\n",
        "from openai import OpenAI\n",
        "import signal\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflzMCzERs1t"
      },
      "source": [
        "## Loading and saving data files from and into JSON or Excel files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QeAPOehRs1u"
      },
      "source": [
        "Functions:\n",
        "- load_data(path) : Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "    - Args:\n",
        "        path (str): the file path\n",
        "    - Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "- save_data(data, path): Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "    - Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "    - Returns:\n",
        "        bool: True if data is successfully saved, False otherwise.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "- merge_benchmarks(folder, excel_output, columns_dict):\n",
        "    Creates a database from all JSON files in the specified folder and its subfolders,\n",
        "    then saves it to an Excel file. If the Excel file already exists, it appends the new data to it.\n",
        "    This is to create the merged excel benchmark that will be used as a base for further operations (call LLMs and classify the ouputs)\n",
        "\n",
        "    Parameters:\n",
        "    - folder (str): The path to the folder containing the JSON files.\n",
        "    - excel_output (str): The file path for the Excel output.\n",
        "    - columns_dict (dict): A dictionary where keys are the desired column names and values are the default values if the column doesn't exist in the JSON data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hB3Ppn3PRs1u"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Loads data from an Excel or JSON file and returns it as a dictionary.\n",
        "\n",
        "    Args:\n",
        "        path (str): the file path\n",
        "\n",
        "    Returns:\n",
        "        dict: data loaded as a dictionary\n",
        "    \"\"\"\n",
        "    data = None\n",
        "\n",
        "    if path.endswith('.xlsx'):\n",
        "        try:\n",
        "            data = pd.read_excel(path).to_dict(orient='records')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from Excel file: {str(e)}\")\n",
        "\n",
        "    elif path.endswith('.json'):\n",
        "        try:\n",
        "            with open(path, 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from JSON file: {str(e)}\")\n",
        "    else:\n",
        "        print(\"Unsupported file format. Please provide an Excel (.xlsx) or JSON (.json) file.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def save_data(data, path):\n",
        "    \"\"\"\n",
        "    Saves data (dictionary or DataFrame) to a JSON or Excel file based on the file extension in the path\n",
        "\n",
        "    Args:\n",
        "        data (dict or DataFrame): data to be saved\n",
        "        path (str): file path where data will be saved\n",
        "\n",
        "    Returns:\n",
        "        bool: True if data is successfully saved, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if path.endswith('.json'):\n",
        "            if isinstance(data, dict):\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data, json_file, indent=2)\n",
        "            elif isinstance(data, pd.DataFrame):\n",
        "                data_dict = data.to_dict(orient='list')\n",
        "                with open(path, 'w') as json_file:\n",
        "                    json.dump(data_dict, json_file, indent=2)\n",
        "\n",
        "        elif path.endswith('.xlsx'):\n",
        "            if isinstance(data, dict):\n",
        "                # Convert dictionary to DataFrame if it's a dictionary\n",
        "                data = pd.DataFrame.from_dict(data)\n",
        "            data.to_excel(path, index=False)\n",
        "        else:\n",
        "            print(\"Unsupported file format. Please provide a JSON (.json) or Excel (.xlsx) file path.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def merge_benchmarks(folder, excel_output, columns_dict):\n",
        "    \"\"\"\n",
        "    Creates a database from all JSON files in the specified folder and its subfolders,\n",
        "    then saves it to an Excel file. If the Excel file already exists, it appends the new data to it.\n",
        "\n",
        "    Parameters:\n",
        "    - folder (str): The path to the folder containing the JSON files.\n",
        "    - excel_output (str): The file path for the Excel output.\n",
        "    - columns_dict (dict): A dictionary where keys are the desired column names and values are the default values if the column doesn't exist in the JSON data.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    # Load existing data if the Excel file exists\n",
        "    if os.path.exists(excel_output):\n",
        "        existing_data = load_data(excel_output)\n",
        "        if existing_data:\n",
        "            all_data.extend(existing_data)\n",
        "\n",
        "    # Process new JSON files\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                data = load_data(file_path)\n",
        "                if data:\n",
        "                    if isinstance(data, list):\n",
        "                        for item in data:\n",
        "                            # Ensure specific columns are included with default values if not present\n",
        "                            for col, default in columns_dict.items():\n",
        "                                item.setdefault(col, default)\n",
        "                            all_data.append(item)\n",
        "                    elif isinstance(data, dict):\n",
        "                        for item in data.values():\n",
        "                            # Ensure specific columns are included with default values if not present\n",
        "                            for col, default in columns_dict.items():\n",
        "                                item.setdefault(col, default)\n",
        "                            all_data.append(item)\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    answers_df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    save_data(answers_df, excel_output)\n",
        "\n",
        "    print(f\"Excel file concatenating all answers created: {excel_output}\\n ----- \\n\")\n",
        "\n",
        "    return answers_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kiKb7ecfKpW"
      },
      "source": [
        "# Query LLMs on the benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZdVoER1Rs2j"
      },
      "source": [
        "Functions:\n",
        "- question_LLM_on_benchmark(LLM_function, benchmark_with_intros, answers_path):\n",
        "    Gets all the LLM answers for an entire benchmark file\n",
        "\n",
        "    Args:\n",
        "        LLM_function (fct): the LLM function to be used;\n",
        "        benchmark_with_intros (str): path of the benchmark\n",
        "        answers_path (str): path of the answers file\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "- create_response_GPT_base, create_response_GPT4o, create_response_X:\n",
        "    functions returning an LLM response from the selected model\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "- get_llm_function(LLM_name, LLM_dict=LLM_dict):\n",
        "    Gets the corresponding LLM function based on the LLM name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cisS1YhvYPWD"
      },
      "outputs": [],
      "source": [
        "def question_LLM_on_benchmark(LLM_function, benchmark_with_intros, answers_path, questions_list=None):\n",
        "    \"\"\"Gets all the LLM answers for a subset of questions in one benchmark file based on their positions, or all questions if questions_list is not provided.\n",
        "\n",
        "    Args:\n",
        "        LLM_function (function): the LLM function to be used; One of the following: create_response_GPT3, create_response_GPT4, create_response_Palm2\n",
        "        benchmark_with_intros (str): path of the benchmark\n",
        "        answers_path (str): path of the answers file\n",
        "        questions_list (list, optional): list of positions of the questions to be processed. If None (default), processes all questions.\n",
        "    \"\"\"\n",
        "\n",
        "    answers = {}\n",
        "    data = load_data(benchmark_with_intros)\n",
        "\n",
        "    # Determine which keys to iterate over based on questions_list\n",
        "    if questions_list is None:\n",
        "        keys_to_process = data.keys()\n",
        "    else:\n",
        "        # Ensure questions_list contains strings for keys and filter out any indices not present in the data\n",
        "        keys_to_process = [str(q) for q in questions_list if str(q) in data]\n",
        "\n",
        "    for key in keys_to_process:\n",
        "        value = data[key]\n",
        "\n",
        "        response_llm = LLM_function(value[\"question\"])\n",
        "\n",
        "        new_instance = {\n",
        "            \"question\": value[\"question\"],\n",
        "            \"choices\": value[\"choices\"],\n",
        "            \"right_answer\": value[\"right_answer\"],\n",
        "            \"llm_answer\": response_llm\n",
        "        }\n",
        "        if \"original_question\" in value:\n",
        "            new_instance[\"original_question\"] = value[\"original_question\"]\n",
        "\n",
        "        answers[key] = new_instance\n",
        "\n",
        "    save_data(answers, answers_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def create_response_GPT_base(question_prompt, basemodel, api_key=OPENAI_API_KEY):\n",
        "    \"\"\"Base function for getting responses from GPT models.\n",
        "\n",
        "    Args:\n",
        "        question_prompt (str): The question prompt\n",
        "        basemodel (str): The model to use (e.g., \"gpt-4o\")\n",
        "        api_key (str, optional): OpenAI API key. Defaults to OPENAI_KEY.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response\n",
        "    \"\"\"\n",
        "    logger.info(f\"--- create_response base : {basemodel} --- \")\n",
        "\n",
        "    response = None\n",
        "    retries = 0\n",
        "\n",
        "    while retries < 3:\n",
        "        try:\n",
        "            client = OpenAI(api_key=api_key)\n",
        "\n",
        "            resp = client.chat.completions.create(\n",
        "                model=basemodel,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                seed=1,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": question_prompt}\n",
        "                ]\n",
        "            )\n",
        "            print(\"tokens : \", resp.usage.total_tokens)\n",
        "            print(\"model used : \", resp.model)\n",
        "            response = resp.choices[0].message.content\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "            print(question_prompt)\n",
        "            time.sleep(0.100)\n",
        "            response = f\"Error occurred: {e}\"\n",
        "            retries += 1\n",
        "\n",
        "    return response\n",
        "\n",
        "def create_response_GPT4o(question_prompt, api_key=OPENAI_API_KEY):\n",
        "    return create_response_GPT_base(question_prompt, \"gpt-4o\", api_key)\n",
        "\n",
        "def create_response_GPT3(question_prompt, api_key=OPENAI_API_KEY):\n",
        "    return create_response_GPT_base(question_prompt, \"gpt-3.5-turbo\", api_key)\n",
        "\n",
        "# create_response_GPT4o(\"what is 2+2?\")\n",
        "\n",
        "LLM_dict = {\"GPT3\" : {\"LLM_function\" :create_response_GPT3},\n",
        "            \"GPT4o\" : {\"LLM_function\" :create_response_GPT4o},\n",
        "                     }\n",
        "                     \n",
        "def get_llm_function(LLM_name, LLM_dict=LLM_dict):\n",
        "    \"\"\"Get the corresponding LLM function based on the LLM string.\n",
        "\n",
        "    Args:\n",
        "        llm_string (str): The LLM string.\n",
        "\n",
        "    Returns:\n",
        "        function: The corresponding LLM function.\n",
        "    \"\"\"\n",
        "    if LLM_name in LLM_dict:\n",
        "        return LLM_dict[LLM_name][\"LLM_function\"]\n",
        "    else:\n",
        "        raise ValueError(f\"No LLM function found for LLM '{LLM_name}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch on selected benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Functions:\n",
        "- randomly_select_lines(answers_input, answers_output, num_personas_per_category=1, selected_string=\"1\"):\n",
        "    Randomly marks  X personas per prompt_characteristic category from the answers file so that only these are processed;\n",
        "    This is used specifically to launch the LLM on a subset of the answers, to save time and money, and to have a representative sample when there are multiple personas per prompt_characteristic category.\n",
        "    \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "- process_selected_rows(excel_file_path, output_excel_file_path, model_name, randomly_selected_value=\"1\", save_interval=10, backup_interval=100, selected_benchmarks=None, selected_prompt_characteristics=None):\n",
        "    Gets LLM responses for the selected questions. This function saves the file periodically and creates backup files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def randomly_select_lines(answers_input, answers_output, num_personas_per_category=1, selected_string=\"1\"):\n",
        "    \"\"\"\n",
        "    Randomly marks  X personas per prompt_characteristic category from the answers file so that only these are processed;\n",
        "\n",
        "    Args:\n",
        "        answers_input (str): The path to the answers input (Excel file).\n",
        "        answers_output (str): The path to the answers output (Excel file).\n",
        "        num_personas_per_category (int): Number of personas to randomly select per prompt_characteristic category.\n",
        "        selected_string (str): The string to add to the 'randomly_selected' column for selected lines.\n",
        "    \"\"\"\n",
        "    # Load the excel answer file\n",
        "    df = pd.read_excel(answers_input)\n",
        "\n",
        "    # Check if the 'randomly_selected' column exists\n",
        "    if 'randomly_selected' in df.columns:\n",
        "        # Exclude rows that already have the 'randomly_selected' column filled with the specified string\n",
        "        df = df[df['randomly_selected'].isna() | (df['randomly_selected'] != selected_string)]\n",
        "\n",
        "    # Initialize a list to hold the indices of the selected rows\n",
        "    selected_indices = []\n",
        "\n",
        "    unique_combinations = df[['question_id', 'prompt_characteristic']].drop_duplicates()\n",
        "\n",
        "    for _, row in unique_combinations.iterrows():\n",
        "        question_id = row['question_id']\n",
        "        prompt_characteristic = row['prompt_characteristic']\n",
        "\n",
        "        # Filter the dataframe to get rows matching the current combination\n",
        "        subset_df = df[(df['question_id'] == question_id) & (df['prompt_characteristic'] == prompt_characteristic)]\n",
        "\n",
        "        # Select the specified number of random personas for this combination\n",
        "        if not subset_df.empty:\n",
        "            num_to_sample = min(num_personas_per_category, len(subset_df))\n",
        "            selected_indices.extend(subset_df.sample(n=num_to_sample).index)\n",
        "\n",
        "    # Update the 'randomly_selected' column for the selected lines\n",
        "    df.loc[selected_indices, 'randomly_selected'] = selected_string\n",
        "\n",
        "    # Save the updated DataFrame back to the Excel file\n",
        "    df.to_excel(answers_output, index=False)\n",
        "    print(f\"Randomly selected personas and updated the answers file: {answers_output}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutException\n",
        "\n",
        "processed_indices = set()\n",
        "\n",
        "def process_selected_rows(excel_file_path, output_excel_file_path, model_name, randomly_selected_value=\"1\", save_interval=10, backup_interval=100, selected_benchmarks=None, selected_prompt_characteristics=None):\n",
        "    \"\"\"\n",
        "    Processes the rows in the Excel file where `randomly_selected` equals the specified value,\n",
        "    and generates responses using the specified LLM model. Saves the file periodically and creates backup files.\n",
        "\n",
        "    Args:\n",
        "        excel_file_path (str): The path to the input Excel file.\n",
        "        output_excel_file_path (str): The path to save the updated Excel file.\n",
        "        model_name (str): The name of the LLM model to use.\n",
        "        randomly_selected_value (str): The value in the `randomly_selected` column to filter by. Default is \"1\".\n",
        "        save_interval (int): The number of rows to process before saving the file. Default is 10.\n",
        "        backup_interval (int): The number of rows to process before saving a backup file. Default is 100.\n",
        "        selected_benchmarks (list): List of benchmarks to filter by.\n",
        "        selected_prompt_characteristics (list): List of user characteristics to filter by.\n",
        "    \"\"\"\n",
        "    # Load the Excel file into a DataFrame\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "\n",
        "    # Check if the randomly_selected column exists\n",
        "    if 'randomly_selected' not in df.columns:\n",
        "        raise KeyError(\"The 'randomly_selected' column was not found in the Excel file.\")\n",
        "    \n",
        "    print(\"Contents of 'randomly_selected' column:\")\n",
        "    print(df['randomly_selected'].value_counts())\n",
        "\n",
        "    # Check if the columns for the model exist, if not create them\n",
        "    answer_column = f'{model_name}_answer'\n",
        "    is_correct_column = f'{model_name}_is_correct'\n",
        "    date_column = f'date_of_launch_{model_name}'\n",
        "    \n",
        "    if answer_column not in df.columns:\n",
        "        df[answer_column] = None\n",
        "    if is_correct_column not in df.columns:\n",
        "        df[is_correct_column] = None\n",
        "    if date_column not in df.columns:\n",
        "        df[date_column] = None\n",
        "\n",
        "    # If selected_benchmarks is empty, select all unique benchmarks\n",
        "    if not selected_benchmarks:\n",
        "        selected_benchmarks = df['benchmark'].unique().tolist()\n",
        "\n",
        "    # If selected_prompt_characteristics is empty, select all unique user characteristics\n",
        "    if not selected_prompt_characteristics:\n",
        "        selected_prompt_characteristics = df['prompt_characteristic'].unique().tolist()\n",
        "\n",
        "    # Filter rows where `randomly_selected` equals the specified value\n",
        "    selected_rows = df[\n",
        "        (df['randomly_selected'] == float(randomly_selected_value)) &\n",
        "        (df['benchmark'].isin(selected_benchmarks)) &\n",
        "        (df['prompt_characteristic'].isin(selected_prompt_characteristics))\n",
        "    ]\n",
        "\n",
        "    # Print the number of selected rows for debugging\n",
        "    print(f\"Number of selected rows: {len(selected_rows)}\")\n",
        "    unanswered_questions = selected_rows[selected_rows[answer_column].isna()]\n",
        "    print(f\"Number of questions left to answer: {len(unanswered_questions)}\")\n",
        "\n",
        "\n",
        "    # Get the corresponding LLM function\n",
        "    llm_function = get_llm_function(model_name)\n",
        "\n",
        "    # Process each selected row\n",
        "    for i, (index, row) in enumerate(selected_rows.iterrows()):\n",
        "        unanswered_questions = selected_rows[selected_rows[answer_column].isna()]\n",
        "        unanswered_questions = selected_rows[selected_rows[answer_column].isna()]\n",
        "        if index in processed_indices:\n",
        "            continue\n",
        "        if pd.isna(row[answer_column]):  # Only process if the answer column is empty\n",
        "            question_prompt = row['question']  # Assuming there's a column 'question' with the prompt\n",
        "            print(f\"Processing row {i + 1}/{len(selected_rows)}\")\n",
        "            #print(f\"Processing row {i + 1}/{len(unanswered_questions)}\")\n",
        "\n",
        "            try:\n",
        "                response = llm_function(question_prompt)\n",
        "            except TimeoutException:\n",
        "                # Save the file before restarting\n",
        "                df.to_excel(output_excel_file_path, index=False)\n",
        "                return  # Exit the current function call after saving\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred: {e}\")\n",
        "                response = None\n",
        "\n",
        "            if response and isinstance(response, str) and response.startswith(\"Error occurred:\"):\n",
        "                response = None\n",
        "            df.at[index, answer_column] = response\n",
        "            df.at[index, is_correct_column] = \"To be determined\"\n",
        "            df.at[index, date_column] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Mark the row as processed\n",
        "            processed_indices.add(index)\n",
        "\n",
        "            # Save the DataFrame every `save_interval` rows\n",
        "            if (i + 1) % save_interval == 0:\n",
        "                df.to_excel(output_excel_file_path, index=False)\n",
        "                print(f\"Saved progress to {output_excel_file_path} after processing {i + 1} rows.\")\n",
        "                print(f\"Number of questions left to answer: {len(unanswered_questions)}\")\n",
        "\n",
        "            # Save a backup file every `backup_interval` rows\n",
        "            if (i + 1) % backup_interval == 0:\n",
        "                backup_file_path = output_excel_file_path.replace(\".xlsx\", f\"_backup_{i + 1}.xlsx\")\n",
        "                df.to_excel(backup_file_path, index=False)\n",
        "                print(f\"Saved backup to {backup_file_path} after processing {i + 1} rows.\")\n",
        "\n",
        "    df.to_excel(output_excel_file_path, index=False)\n",
        "    print(f\"Final save completed. Updated Excel file saved to {output_excel_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkgfpb6qvAd3"
      },
      "source": [
        "# Step 3 Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "folder_path = \"YOUR_FOLDER_PATH\"\n",
        "\n",
        "# Example run\n",
        "folder_modified_benchmarks_step2 = folder_path + \"step2_modified-benchmarks/\"\n",
        "folder_answered_benchmarks_step3 = folder_path + \"step3_answered-benchmarks/\"\n",
        "modified_benchmark_path = folder_modified_benchmarks_step2 + \"strategyQA_base_base.json\"\n",
        "merged_benchmarks_path = folder_modified_benchmarks_step2 + \"merged_benchmarks.xlsx\"\n",
        "\n",
        "LLM_dict = {\"GPT3\" : {\"LLM_function\" :create_response_GPT3},\n",
        "            \"GPT4o\" : {\"LLM_function\" :create_response_GPT4o},\n",
        "                     }\n",
        "\n",
        "\n",
        "merge_benchmarks(folder_modified_benchmarks_step2, merged_benchmarks_path, {\"benchmark_name\": \"\", \"question_id\": \"\", \"question\": \"\", \"choices\": None, \"right_answer\": \"\", \"original_question\": \"\", \"llm_answer\": \"\"}) \n",
        "# here, the second part of the dictionary in argument is for the default value of the columns if there is no value indicated i the JSON files; for instance, if there are no choices, the default value for the Choices columnwill be None \n",
        "\n",
        "randomly_select_lines(merged_benchmarks_path, merged_benchmarks_path, num_personas_per_category=1, selected_string=\"1\")\n",
        "\n",
        "model_name = 'GPT4o'\n",
        "process_selected_rows(merged_benchmarks_path, merged_benchmarks_path, model_name, randomly_selected_value=\"1\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
